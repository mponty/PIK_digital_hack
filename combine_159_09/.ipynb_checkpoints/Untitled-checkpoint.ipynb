{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import gc\n",
    "sys.path.append('/usr/local/lib/python3.6/site-packages')\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from datetime import datetime\n",
    "\n",
    "from sklearn.model_selection import train_test_split, KFold, StratifiedKFold, RepeatedStratifiedKFold\n",
    "from sklearn.metrics import roc_auc_score, mean_squared_error, make_scorer\n",
    "from sklearn.linear_model import LogisticRegression, LogisticRegressionCV\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "\n",
    "import lightgbm as lgb\n",
    "#from hyperopt import hp, tpe\n",
    "#from hyperopt.fmin import fmin\n",
    "\n",
    "gc.collect()\n",
    "\n",
    "\n",
    "def mem_economy(dataset):\n",
    "    features = np.array(dataset.dtypes[dataset.dtypes=='float64'].index)\n",
    "    for f in features:\n",
    "        dataset[f] = dataset[f].astype('float32')\n",
    "    features = np.array(dataset.dtypes[dataset.dtypes=='int64'].index)\n",
    "    for f in features:\n",
    "        dataset[f] = dataset[f].astype('int32')\n",
    "    \n",
    "    return dataset\n",
    "\n",
    "def my_drop_levels(dataset, sep = '_', brief = ''):\n",
    "    if dataset.columns.nlevels>1:\n",
    "        new_columns = np.array([], dtype = 'str')\n",
    "        for col_i in range(dataset.shape[1]):\n",
    "            col_name = brief\n",
    "            for level in range(dataset.columns.nlevels):\n",
    "                tmp_col_name = dataset.columns.levels[level][dataset.columns.labels[level][col_i]]#.astype('str')\n",
    "                tmp_col_name = str(tmp_col_name) \n",
    "                if (level>0) & (tmp_col_name!=''):\n",
    "                    col_name = col_name+'_'\n",
    "                col_name = col_name+tmp_col_name\n",
    "            new_columns = np.append(new_columns,col_name)\n",
    "            #print(col_name)\n",
    "        for level in range(dataset.columns.nlevels-1):\n",
    "            dataset.columns.droplevel(0)\n",
    "        dataset.columns = new_columns      \n",
    "    return dataset\n",
    "\n",
    "def prepare_full():\n",
    "    train = pd.read_csv('../files-pik_digital_day/train.csv', encoding='cp1251')\n",
    "    test = pd.read_csv('../files-pik_digital_day/test.csv', encoding='cp1251')\n",
    "    \n",
    "    train['is_train'] = 1\n",
    "    test['is_train'] = 0\n",
    "    \n",
    "    full = pd.concat([train,test])\n",
    "    \n",
    "    del train, test\n",
    "    gc.collect()\n",
    "    \n",
    "    full = full.sort_values('month_cnt')\n",
    "    \n",
    "    le = LabelEncoder()\n",
    "    full['bulk_id_int'] = le.fit_transform(full['bulk_id'])\n",
    "    full['date1'] = pd.to_datetime(full['date1'], format='%Y-%m-%d')\n",
    "    GLOBAL_MINDATE = full['date1'].min()\n",
    "    full['Date_int'] = ((full['date1'] - GLOBAL_MINDATE)/np.timedelta64(1, 'D')).astype('int32')\n",
    "\n",
    "    full['Автомойка'] = (full['Автомойка']=='да').astype('int')\n",
    "    full['Входные группы'] = (full['Входные группы']=='да').astype('int')\n",
    "    full['Двор без машин'] = (full['Двор без машин']=='да').astype('int')\n",
    "    full['Класс объекта'] = full['Класс объекта'].map({'эконом':1, 'комфорт':3, 'стандарт':2})\n",
    "    full['Кладовые'] = (full['Кладовые']=='да').astype('int')\n",
    "    full['Колясочные'] = (full['Колясочные']=='да').astype('int')\n",
    "    full['Огорожена территория'] = (full['Огорожена территория']=='да').astype('int')\n",
    "    full['Подземная парковка'] = (full['Подземная парковка']=='да').astype('int')\n",
    "    full['Система мусоротведения'] = le.fit_transform(full['Система мусоротведения'])\n",
    "    full['Спортивная площадка'] = (full['Спортивная площадка']=='да').astype('int')\n",
    "    \n",
    "    #введем уникальные id\n",
    "    full['bulk_spalen_id'] = full['bulk_id_int'].astype('str')+'_'+full['spalen'].astype('str')\n",
    "    full['bulk_spalen_id'] = le.fit_transform(full['bulk_spalen_id'])\n",
    "     \n",
    "    # подсчитаем псевдо start_square (без учета возвращенных)\n",
    "    full['calc_start_square'] = full.groupby(['bulk_spalen_id'])['start_square'].shift(1) - full.groupby(['bulk_spalen_id'])['value'].shift(1)\n",
    "    full['calc_last_value'] = full.groupby(['bulk_spalen_id'])['value'].shift(1)\n",
    "    \n",
    "    full['date2'] = full.date1+ pd.offsets.MonthEnd(1)\n",
    "    \n",
    "    full['price_by_square'] = full['price']/full['mean_sq']\n",
    "    \n",
    "    \n",
    "    full = full.reset_index(drop = True)\n",
    "    \n",
    "    return full\n",
    "    \n",
    "\n",
    "def prepare_flat():\n",
    "    le = LabelEncoder()\n",
    "    flat = pd.read_csv('../files-pik_digital_day/flat.csv', encoding='cp1251')\n",
    "    flat = flat.rename(columns = {'id_bulk':'bulk_id'})\n",
    "    flat['id_flatwork_int'] = np.array(flat.index).astype('int')\n",
    "    \n",
    "    dict_bulk_spalen = full.loc[:, ('bulk_id','bulk_id_int','spalen','bulk_spalen_id')] \\\n",
    "                       .drop_duplicates() \n",
    "    dict_flat = flat[['id_flatwork_int','id_flatwork','bulk_id','spalen']].copy()\n",
    "\n",
    "    dict_flat = dict_flat.merge(dict_bulk_spalen, how = 'left')\n",
    "    \n",
    "    \n",
    "    flat['Автомойка'] = (flat['Автомойка']=='да').astype('int')\n",
    "    flat['Входные группы'] = (flat['Входные группы']=='да').astype('int')\n",
    "    flat['Двор без машин'] = (flat['Двор без машин']=='да').astype('int')\n",
    "    \n",
    "\n",
    "    flat['Класс объекта'] = flat['Класс объекта'].fillna('эконом').map({'эконом':1, 'комфорт':3, 'стандарт':2}).astype('int')\n",
    "    flat['Кладовые'] = (flat['Кладовые']=='да').astype('int')\n",
    "    flat['Колясочные'] = (flat['Колясочные']=='да').astype('int')\n",
    "    flat['Огорожена территория'] = (flat['Огорожена территория']=='да').astype('int')\n",
    "    flat['Подземная парковка'] = (flat['Подземная парковка']=='да').astype('int')\n",
    "    flat.drop('Система мусоротведения', axis = 1, inplace = True)\n",
    "    #flat['Система мусоротведения'] = le.fit_transform(flat['Система мусоротведения'])\n",
    "    flat['Спортивная площадка'] = (flat['Спортивная площадка']=='да').astype('int')\n",
    "    flat['otdelka'] = le.fit_transform(flat['otdelka'].fillna('nan'))\n",
    "    flat['vid'] = flat['vid'].map({'эконом':1, 'средний':2, 'хороший':3}).fillna(0).astype('int')\n",
    "    flat['plan_size'] = flat['plan_size'].fillna('-1').map({'S':1, 'M':2, 'L':3, '-1':0}).astype('int')\n",
    "    flat['plan0'] = le.fit_transform(flat['plan0'].fillna('nan'))\n",
    "    \n",
    "\n",
    "    flat['date_flat_startsale'] = pd.to_datetime(flat['flat_startsale'].fillna('2018-03-01'), format='%Y-%m-%d')\n",
    "    flat['date_settle'] = pd.to_datetime(flat['date_settle'], format='%Y-%m-%d')\n",
    "    flat['date_salestart'] = pd.to_datetime(flat['date_salestart'].fillna('2018-03-01'), format='%Y-%m-%d')\n",
    "    \n",
    "    flat['dt_flat_salestart_delay'] = ((flat['date_flat_startsale'] - flat['date_salestart'])/np.timedelta64(1, 'D')).astype('int32') \n",
    "    \n",
    "    \n",
    "    flat['sale'] = (pd.to_datetime(flat['sale'], format = '%Y-%m-%d %H:%M:%S') +  np.timedelta64(1,'D') \n",
    "                   ).dt.date\n",
    "\n",
    "\n",
    "    flat.loc[~flat['date_settle'].isna(),'dt_settle_salestart'] = ((flat.loc[~flat['date_settle'].isna(),'date_settle'] - flat.loc[~flat['date_settle'].isna(),'date_salestart'])/np.timedelta64(1, 'D')).astype('int32')\n",
    "\n",
    "    #заполним медианным значением\n",
    "    dt_settle_salestart_median = int(flat.loc[~flat['date_settle'].isna(),'dt_settle_salestart'].median())\n",
    "    flat.loc[flat['date_settle'].isna(),'dt_settle_salestart'] = dt_settle_salestart_median  \n",
    "    \n",
    "    flat['dt_settle_salestart'] = flat['dt_settle_salestart'].astype('int32')\n",
    "    \n",
    "    flat.loc[flat['date_settle'].isna(),'date_settle'] = flat.loc[flat['date_settle'].isna(),'date_salestart'] + np.timedelta64(dt_settle_salestart_median, 'D')\n",
    "    \n",
    "    \n",
    "    flat = flat.merge(dict_flat[['id_flatwork_int','bulk_spalen_id','bulk_id_int']], how = 'left', on = 'id_flatwork_int')\n",
    "     \n",
    "    return flat, dict_bulk_spalen, dict_flat\n",
    "    \n",
    "def prepare_status():\n",
    "    \n",
    "\n",
    "\n",
    "    status = pd.read_csv('../files-pik_digital_day/status.csv', encoding='cp1251')\n",
    "    status = status.merge(dict_flat, how = 'inner')\n",
    "\n",
    "    #удалим статусы-однодневки\n",
    "    status = status[status['datefrom']!=status['dateto']]\n",
    "\n",
    "    dict_stat = status[['stat','stat_name']].drop_duplicates()\n",
    "    dict_stat['can_be_sold'] = (~dict_stat.stat_name.isin(['Реализован','Статус после покупки'])).astype('int16')\n",
    "    dict_stat['sold'] = (dict_stat.stat_name.isin(['Реализован','Статус после покупки'])).astype('int16')\n",
    "    dict_stat['realize'] = (dict_stat.stat_name.isin(['Реализован'])).astype('int16')\n",
    "    dict_stat['stat_new'] = dict_stat['stat_name'].map({'Не реализуется':0,\n",
    "                                                        'В реализации (не на сайте)':1,\n",
    "                                                        'В реализации':2,\n",
    "                                                        'Онлайн бронирование':3,\n",
    "                                                        'Зарезервирован под клиента':3,\n",
    "                                                        'Платное бронирование':4,\n",
    "                                                        'Реализован':5,\n",
    "                                                        'Статус после покупки':6\n",
    "                                                        }).astype('int16')\n",
    "\n",
    "    status = status.merge(dict_stat[['stat','stat_new']], how = 'inner')\n",
    "\n",
    "\n",
    "    status['datefrom'] = (pd.to_datetime(status['datefrom'], format = '%Y-%m-%d %H:%M:%S') + np.timedelta64(1,'D')).dt.date.astype('str')\n",
    "    status['dateto'] = (pd.to_datetime(status['dateto'], format = '%Y-%m-%d %H:%M:%S') + np.timedelta64(1,'D')).dt.date.astype('str')\n",
    "\n",
    "\n",
    "    status['datefrom_dt'] = pd.to_datetime(status['datefrom'], format='%Y-%m-%d')\n",
    "    status['dateto_dt'] = pd.to_datetime(status['dateto'], format='%Y-%m-%d')\n",
    "\n",
    "\n",
    "    status = status.sort_values(['datefrom','dateto'])\n",
    "    status['last_stat_new'] = status.groupby(['id_flatwork_int'])['stat_new'].shift(1)#.fillna('stat_new')\n",
    "    status.loc[status['last_stat_new'].isna(),'last_stat_new'] = -1\n",
    "\n",
    "\n",
    "\n",
    "    status['delay'] = (status['dateto_dt']-status['datefrom_dt'])/np.timedelta64(1,'D')\n",
    "\n",
    "\n",
    "    gc.collect() \n",
    "    \n",
    "    return status, dict_stat\n",
    "    \n",
    "def prepare_price():\n",
    "    \n",
    "    price = pd.read_csv('../files-pik_digital_day/price.csv', encoding='utf-8')\n",
    "    price = price.merge(dict_flat, how = 'inner')\n",
    "\n",
    "    #удалим пустые цены и цены однодневки\n",
    "    price = price[(price.pricem2>1) & (price['datefrom']!=price['dateto'])].sort_values(['datefrom','dateto'])\n",
    "\n",
    "    price['datefrom'] = (pd.to_datetime(price['datefrom'], format = '%Y-%m-%d %H:%M:%S') + np.timedelta64(1,'D')).dt.date.astype('str')\n",
    "    price['dateto'] = (pd.to_datetime(price['dateto'], format = '%Y-%m-%d %H:%M:%S') + np.timedelta64(1,'D')).dt.date.astype('str')\n",
    "\n",
    "\n",
    "    price['last_pricem2'] = price.groupby(['id_flatwork_int'])['pricem2'].shift(1).fillna(0)\n",
    "    price['diff_pricem2'] = price['pricem2'] - price['last_pricem2']\n",
    "    price['was_decrease'] = (price['diff_pricem2'] < 0).astype('int32')\n",
    "\n",
    "\n",
    "\n",
    "    price['datefrom_dt'] = pd.to_datetime(price['datefrom'], format='%Y-%m-%d')\n",
    "    price['dateto_dt'] = pd.to_datetime(price['dateto'], format='%Y-%m-%d')\n",
    "\n",
    "    price = price.merge(flat.loc[:,('sale','id_flatwork_int')], how = 'left')\n",
    "    price['sale'] = pd.to_datetime(price['sale'], format='%Y-%m-%d')\n",
    "    price['is_saled_price'] = ((price['sale']>=price['datefrom_dt']) & (price['sale']<price['dateto_dt'])).astype('int')\n",
    "\n",
    "    price['delay'] = (price['dateto_dt']-price['datefrom_dt'])/np.timedelta64(1,'D')\n",
    "\n",
    "    gc.collect()\n",
    "    \n",
    "    return price   \n",
    "\n",
    "def prepare_flat_train(test_days_period):\n",
    "    \n",
    "\n",
    "    fixed_dates = np.sort(full.date1.dt.strftime('%Y-%m-%d').unique())\n",
    "    fixed_dates_last = np.sort(full.date2.dt.strftime('%Y-%m-%d').unique())\n",
    "\n",
    "\n",
    "    for i in range(len(fixed_dates)):\n",
    "        gc.collect()\n",
    "\n",
    "\n",
    "        fixed_date = fixed_dates[i] \n",
    "        fixed_date_last = fixed_dates_last[i]\n",
    "\n",
    "\n",
    "        #найдем квартиры, доступные к продаже на эту дату\n",
    "        status_on_date = status[(status.datefrom<=fixed_date) & (status.dateto>fixed_date)].copy()\n",
    "\n",
    "\n",
    "        #срок жизни статуса\n",
    "        status_on_date['datefrom'] = pd.to_datetime(status_on_date['datefrom'], format='%Y-%m-%d')\n",
    "        status_on_date['datenow'] = pd.to_datetime(fixed_date, format='%Y-%m-%d')\n",
    "        #status_on_date['dateto'] = pd.to_datetime(status_on_date['dateto'], format='%Y-%m-%d %H:%M:%S')\n",
    "\n",
    "        status_on_date['status_days'] = ((status_on_date['datenow'] - \n",
    "                                          status_on_date['datefrom'])/np.timedelta64(1, 'D')).astype('int32')\n",
    "\n",
    "        #подсчитаем все статусы, которые были у этой квартиры к указанной дате\n",
    "        statuses_to_date = status[(status.datefrom<=fixed_date)] \\\n",
    "                        .groupby(['id_flatwork_int','stat_new']) \\\n",
    "                        .size() \\\n",
    "                        .reset_index(name = 'cnt_stat_new')\n",
    "        statuses_to_date = statuses_to_date.pivot(index = 'id_flatwork_int', columns='stat_new').fillna(0)\n",
    "        statuses_to_date = my_drop_levels(statuses_to_date, sep = '_') \n",
    "        statuses_to_date = statuses_to_date.reset_index()\n",
    "\n",
    "        #удалим задвоения\n",
    "        tmp = status_on_date.groupby('id_flatwork_int').size().reset_index(name = 'cnt')\n",
    "        flats_to_delete =  tmp.loc[tmp['cnt']>1,'id_flatwork_int']\n",
    "        status_on_date = status_on_date[~status_on_date.id_flatwork_int.isin(flats_to_delete)] \n",
    "\n",
    "        #квартиры, которые могут быть проданы в этом периоде\n",
    "        stats_can_be_sold = dict_stat[dict_stat['can_be_sold']==1].stat\n",
    "        flats_can_be_sold = np.array(status_on_date[status_on_date.stat.isin(stats_can_be_sold)] \\\n",
    "                                     .id_flatwork_int)\n",
    "\n",
    "        #  \n",
    "        stats_sold = dict_stat[dict_stat['sold']==1].stat\n",
    "        flats_sold = status_on_date[status_on_date.stat.isin(stats_sold)].id_flatwork_int\n",
    "        \n",
    "        \n",
    "        flats_returned = np.array(flat[(flat.sale.astype('str')>fixed_date) & \n",
    "                              flat.id_flatwork_int.isin(flats_sold)].id_flatwork_int)\n",
    "        \n",
    "        flats_can_be_sold = np.append(flats_can_be_sold, flats_returned)\n",
    "        \n",
    "        #формируем простейшую поквартирную обучающую выборку\n",
    "        #Если реальная дата продажи > чем начало периода, то даже при статусе реализован, она может быть продана\n",
    "        tmp_flat_train = flat[(~flat.bulk_spalen_id.isna()) & \n",
    "                              (flat.date_salestart <= fixed_date_last) & #ВОТ ТУТ ПОМЕНЯТЬ!!!\n",
    "                              #(flat.flat_salestart <= fixed_date_last) \n",
    "                              (flat.id_flatwork_int.isin(flats_can_be_sold))].copy()\n",
    "\n",
    "        tmp_flat_train['date1']=fixed_date \n",
    "        tmp_flat_train['month_cnt']=i\n",
    "        tmp_flat_train['dt_to_settle'] = ((pd.to_datetime(tmp_flat_train['date1'], \n",
    "                                                          format='%Y-%m-%d')  - \n",
    "                                           tmp_flat_train['date_settle'])/np.timedelta64(1, 'D')).astype('int32')\n",
    "        tmp_flat_train['dt_to_salestart'] = ((pd.to_datetime(tmp_flat_train['date1'], format='%Y-%m-%d') - \n",
    "                                              tmp_flat_train['date_salestart'])/np.timedelta64(1, 'D')).astype('int32')\n",
    "\n",
    "        tmp_flat_train['dt_to_sale'] = ((pd.to_datetime(tmp_flat_train['sale'], \n",
    "                                                        format='%Y-%m-%d') - \n",
    "                                         pd.to_datetime(tmp_flat_train['date1'], \n",
    "                                                        format='%Y-%m-%d'))/np.timedelta64(1, 'D')).astype('int32')  \n",
    "\n",
    "        tmp_flat_train = tmp_flat_train.merge(status_on_date[['id_flatwork_int','stat_new',\n",
    "                                                              'last_stat_new','status_days']], \n",
    "                                              how = 'inner', \n",
    "                                              on = 'id_flatwork_int')\n",
    "        tmp_flat_train = tmp_flat_train.merge(statuses_to_date, how = 'left', on = 'id_flatwork_int')\n",
    "\n",
    "        #цена на дату\n",
    "        price_on_date = price[(price.datefrom<=fixed_date) & (price.dateto>fixed_date)].copy()\n",
    "        price_on_date['datefrom'] = pd.to_datetime(price_on_date['datefrom'], format='%Y-%m-%d')\n",
    "        price_on_date['datenow'] = pd.to_datetime(fixed_date, format='%Y-%m-%d')\n",
    "        #status_on_date['dateto'] = pd.to_datetime(status_on_date['dateto'], format='%Y-%m-%d %H:%M:%S')\n",
    "\n",
    "        price_on_date['price_days'] = ((price_on_date['datenow'] - \n",
    "                                        price_on_date['datefrom'])/np.timedelta64(1, 'D')).astype('int32')\n",
    "\n",
    "\n",
    "        tmp_flat_train = tmp_flat_train.merge(price_on_date[['id_flatwork_int','pricem2',\n",
    "                                                             'last_pricem2','diff_pricem2',\n",
    "                                                             'price_days','was_decrease']], \n",
    "                                              how = 'inner', \n",
    "                                              on = 'id_flatwork_int')\n",
    "\n",
    "\n",
    "        #исторические движения по цене\n",
    "        prices_to_date = price[(price.datefrom<=fixed_date) & (price.pricem2>1)] \\\n",
    "                        .groupby(['id_flatwork_int']) \\\n",
    "                        .agg({'pricem2':('min','max','mean','median','std'),\n",
    "                              'was_decrease':('sum','mean','std')})\n",
    "\n",
    "        prices_to_date = my_drop_levels(prices_to_date, sep = '_') \n",
    "        prices_to_date = prices_to_date.reset_index()\n",
    "        tmp_flat_train = tmp_flat_train.merge(prices_to_date, how = 'left', on = 'id_flatwork_int')\n",
    "\n",
    "\n",
    "        if i==0:\n",
    "            flat_train = tmp_flat_train.fillna(0)\n",
    "        else:\n",
    "            flat_train = flat_train.append(tmp_flat_train.fillna(0))\n",
    "\n",
    "       \n",
    "\n",
    "    flat_train = flat_train.fillna(0) \n",
    "\n",
    "\n",
    "    flat_train['realized_1'] = ((flat_train.dt_to_sale>=0) & \n",
    "                                (flat_train.dt_to_sale<test_days_period[0])).astype('int')\n",
    "    flat_train['realized_2'] = ((flat_train.dt_to_sale>=test_days_period[0]) & \n",
    "                                (flat_train.dt_to_sale<test_days_period[0]+test_days_period[1])).astype('int')           \n",
    "    flat_train['realized_3'] = ((flat_train.dt_to_sale>=test_days_period[0]+test_days_period[1]) & \n",
    "                                (flat_train.dt_to_sale<=test_days_period[0]+test_days_period[1]+test_days_period[2])).astype('int') \n",
    "\n",
    "    flat_train['value_1'] = flat_train['square']*flat_train['realized_1']\n",
    "    flat_train['value_2'] = flat_train['square']*flat_train['realized_2']\n",
    "    flat_train['value_3'] = flat_train['square']*flat_train['realized_3']\n",
    "    \n",
    "    \n",
    "    #может иметь психологический эффект\n",
    "    flat_train['price'] = flat_train['pricem2']*flat_train['square']\n",
    "\n",
    "    tmp = flat_train[(flat_train['pricem2']>1) & (flat_train['stat_new']>0) & (flat_train['stat_new']<5)] \\\n",
    "                    .groupby(['month_cnt','bulk_spalen_id']) \\\n",
    "                    .agg({'price':('min','max','std','count','median'),\n",
    "                          'pricem2':('min','max','std','median')})\n",
    "\n",
    "    tmp = my_drop_levels(tmp, sep = '_', brief = 'bulk_spalen_').reset_index()\n",
    "\n",
    "    flat_train = flat_train.merge(tmp, on = ['month_cnt','bulk_spalen_id'], how = 'left').fillna(0)\n",
    "\n",
    "    del tmp\n",
    "    gc.collect()\n",
    "\n",
    "    flat_train['diff_pricem2_median'] = flat_train['bulk_spalen_pricem2_median']-flat_train['pricem2']\n",
    "    flat_train['diff_price_median'] = flat_train['bulk_spalen_price_median']-flat_train['price']\n",
    "\n",
    "    flat_train['diff_pricem2_min'] = flat_train['pricem2'] - flat_train['bulk_spalen_pricem2_min']\n",
    "    flat_train['diff_price_min'] = flat_train['price'] - flat_train['bulk_spalen_price_min']\n",
    "\n",
    "    flat_train['diff_pricem2_max'] = flat_train['bulk_spalen_pricem2_max']-flat_train['pricem2']\n",
    "    flat_train['diff_price_max'] = flat_train['bulk_spalen_price_max']-flat_train['price']\n",
    "\n",
    "\n",
    "    tmp = flat_train[(flat_train['stat_new']>0) & (flat_train['stat_new']<5)] \\\n",
    "                    .groupby(['month_cnt','bulk_spalen_id']) \\\n",
    "                    .agg({'square':('min','max','std','count','median','mean')})\n",
    "\n",
    "    tmp = my_drop_levels(tmp, sep = '_', brief = 'bulk_spalen_').reset_index()\n",
    "\n",
    "    flat_train = flat_train.merge(tmp, on = ['month_cnt','bulk_spalen_id'], how = 'left').fillna(0)\n",
    "\n",
    "    del tmp\n",
    "    gc.collect()\n",
    "\n",
    "    flat_train['diff_square_median'] = flat_train['bulk_spalen_square_median']-flat_train['square']\n",
    "    flat_train['diff_square_mean'] = flat_train['bulk_spalen_square_mean']-flat_train['square']\n",
    "    flat_train['diff_square_min'] = flat_train['bulk_spalen_square_min']-flat_train['square']\n",
    "    flat_train['diff_square_max'] = flat_train['bulk_spalen_square_max']-flat_train['square']\n",
    "\n",
    "\n",
    "\n",
    "    tmp = flat_train.groupby(['month_cnt','bulk_spalen_id', 'stat_new']) \\\n",
    "                    .size() \\\n",
    "                    .reset_index(name = 'dolya_stat_new')\n",
    "\n",
    "    tmp1 = flat_train.groupby(['month_cnt','bulk_spalen_id']) \\\n",
    "                    .size() \\\n",
    "                    .reset_index(name = 'cnt_stat_new') \n",
    "\n",
    "\n",
    "    tmp1['unique_id'] = tmp1['month_cnt'].astype('str')+'_'+tmp1['bulk_spalen_id'].astype('str')\n",
    "\n",
    "    tmp = tmp.merge(tmp1) \n",
    "    tmp['dolya_stat_new']=tmp['dolya_stat_new']/tmp['cnt_stat_new']\n",
    "\n",
    "\n",
    "    tmp2 = tmp[['unique_id','stat_new','dolya_stat_new']].pivot(index = 'unique_id', columns='stat_new') \\\n",
    "                                                         .fillna(0) \n",
    "\n",
    "    tmp2 = my_drop_levels(tmp2, sep = '_', brief = 'bulk_spalen_').reset_index()\n",
    "    tmp2 = tmp2.merge(tmp1) \n",
    "    #tmp = my_drop_levels(tmp, sep = '_', brief = 'bulk_spalen_').reset_index()\n",
    "\n",
    "    flat_train = flat_train.merge(tmp2.drop('unique_id', axis = 1), on = ['month_cnt','bulk_spalen_id'], how = 'left').fillna(0) \n",
    "\n",
    "    del tmp, tmp1, tmp2\n",
    "    gc.collect()\n",
    "\n",
    "    flat_train['month'] = pd.to_datetime(flat_train.date1, format = '%Y-%m-%d').dt.month\n",
    "    \n",
    "    flat_train = mem_economy(flat_train)\n",
    "    gc.collect()\n",
    "    \n",
    "    \n",
    "    tmp = flat_train.groupby(['month_cnt','bulk_spalen_id', 'stat_new']) \\\n",
    "                    .agg({'square':'sum'}) \\\n",
    "                    .reset_index() \\\n",
    "                    .rename(columns = {'square':'dolya_sqr_stat_new'})\n",
    "\n",
    "    tmp1 = flat_train.groupby(['month_cnt','bulk_spalen_id']) \\\n",
    "                    .agg({'square':'sum'}) \\\n",
    "                    .reset_index() \\\n",
    "                    .rename(columns = {'square':'sqr_stat_new'})\n",
    "\n",
    "\n",
    "    tmp1['unique_id'] = tmp1['month_cnt'].astype('str')+'_'+tmp1['bulk_spalen_id'].astype('str')\n",
    "\n",
    "    tmp = tmp.merge(tmp1) \n",
    "    tmp['dolya_sqr_stat_new']=tmp['dolya_sqr_stat_new']/tmp['sqr_stat_new']\n",
    "\n",
    "\n",
    "    tmp2 = tmp[['unique_id','stat_new','dolya_sqr_stat_new']].pivot(index = 'unique_id', columns='stat_new') \\\n",
    "                                                         .fillna(0) \n",
    "\n",
    "    tmp2 = my_drop_levels(tmp2, sep = '_', brief = 'bulk_spalen_').reset_index()\n",
    "    tmp2 = tmp2.merge(tmp1) \n",
    "    #tmp = my_drop_levels(tmp, sep = '_', brief = 'bulk_spalen_').reset_index()\n",
    "\n",
    "    flat_train = flat_train.merge(tmp2.drop('unique_id', axis = 1), on = ['month_cnt','bulk_spalen_id'], how = 'left').fillna(0) \n",
    "\n",
    "    del tmp, tmp1, tmp2\n",
    "    gc.collect()\n",
    "\n",
    "    return flat_train\n",
    "\n",
    "def my_simple_cv(model, dataset, study_columns, random_state=442, importance_flag = False):\n",
    "    \n",
    "    train_agg = dataset[dataset.is_train==1].copy().reset_index(drop = True)\n",
    "    test_agg = dataset[dataset.is_train==0].copy().reset_index(drop = True)\n",
    "    \n",
    "    ind = 0\n",
    "    _mse = np.array([],dtype = 'float')\n",
    "    #заполним нулями предикт теста\n",
    "    y_test_pred = np.zeros(test_agg.shape[0],dtype = 'float')\n",
    "    \n",
    "    \n",
    "    #основная кросс-валидация\n",
    "    for train_index, valid_index in KFold(n_splits=5, random_state=random_state, shuffle = True).split(train_agg):   \n",
    "\n",
    "        tmp_train  = train_agg.loc[train_index,:]\n",
    "        tmp_valid  = train_agg.loc[valid_index,:]\n",
    "        tmp_test   = test_agg.copy()\n",
    "\n",
    "        #учиться будем только на study_columns не на всех переменных     \n",
    "        X_train = tmp_train.loc[:,study_columns]\n",
    "        X_valid = tmp_valid.loc[:,study_columns]\n",
    "        X_test  = tmp_test.loc[:,study_columns]\n",
    "\n",
    "        y_train = tmp_train['value']\n",
    "        y_valid = tmp_valid['value']\n",
    "        y_test = tmp_test['value'] \n",
    "        \n",
    "                \n",
    "        #обучим модель\n",
    "        model.fit(X_train,y_train)\n",
    " \n",
    "        y_valid_pred = model.predict(X_valid)\n",
    "        y_test_pred = y_test_pred+model.predict(X_test)\n",
    "        \n",
    "        \n",
    "        y_valid_pred[y_valid_pred<0] = 0\n",
    "        \n",
    "        if ind ==0:\n",
    "            stacking_df = pd.DataFrame(dict({'bulk_id_int':tmp_valid.bulk_id_int,'predict':y_valid_pred, 'fact':y_valid}))\n",
    "        else:\n",
    "            tmp_stacking_df = pd.DataFrame(dict({'bulk_id_int':tmp_valid.bulk_id_int,'predict':y_valid_pred, 'fact':y_valid}))\n",
    "            stacking_df = stacking_df.append(tmp_stacking_df).sort_values('bulk_id_int')\n",
    "        \n",
    "        \n",
    "        _mse = np.append(_mse,mean_squared_error(y_valid,y_valid_pred))\n",
    "\n",
    "        ind = ind + 1\n",
    "\n",
    "        #break\n",
    "    \n",
    "    \n",
    "    importance = pd.DataFrame(dict({'feature':'none', 'delta_mse':0}), index = ['none'])\n",
    "    \n",
    "    mse_now = mean_squared_error(y_valid,y_valid_pred)\n",
    "    NUMBER_SHUFFLE = 5\n",
    "    if importance_flag:\n",
    "        for feature in study_columns:\n",
    "\n",
    "            tmp_mse = 0\n",
    "            for i in range(NUMBER_SHUFFLE):\n",
    "                _X_valid = X_valid.copy()\n",
    "                a = np.asarray(X_valid[feature].copy())\n",
    "                np.random.shuffle(a)\n",
    "                _X_valid[feature] = a\n",
    "                y_valid_pred = model.predict(_X_valid)\n",
    "                tmp_mse = tmp_mse+mean_squared_error(y_valid, y_valid_pred)/NUMBER_SHUFFLE\n",
    "            tmp_importance = pd.DataFrame(dict({'feature':feature, 'delta_mse':(tmp_mse-mse_now)}), index = [feature])    \n",
    "            importance = importance.append(tmp_importance) \n",
    "    \n",
    "    \n",
    "    #усредняем по фолдам предсказание теста\n",
    "    y_test_pred = y_test_pred/ind\n",
    "    \n",
    "    y_test_pred[y_test_pred<0] = 0\n",
    "    \n",
    "    submission = pd.DataFrame(dict({'id':test_agg.id,'value':y_test_pred, 'bulk_spalen_id':test_agg.bulk_spalen_id}))\n",
    "    \n",
    "    \n",
    "    \n",
    "    return submission, _mse, stacking_df, importance\n",
    "\n",
    "def my_submit(model, \n",
    "                        dataset, \n",
    "                        right_dataset, \n",
    "                        right_date, \n",
    "                        cv_dates,\n",
    "                        last_date,\n",
    "                        n_month,\n",
    "                        study_columns, \n",
    "                        value_column, \n",
    "                        group_columns, \n",
    "                        random_state=442, \n",
    "                        importance_flag = False):\n",
    "    \n",
    "    #весь обучающий датасет\n",
    "    train_agg = dataset.copy().reset_index(drop = True)\n",
    "    \n",
    "    ind = 0\n",
    "    _mse = np.array([],dtype = 'float')\n",
    "    _grp_mse = np.array([],dtype = 'float')\n",
    "    gc.collect()\n",
    "    print('==========================')\n",
    "    \n",
    "    #основная кросс-валидация\n",
    "    d = cv_dates[len(cv_dates)-1]\n",
    "     \n",
    "    #Расчитаем для submit-а\n",
    "\n",
    "    #обучающая сдвигается на 1 месяц вперед\n",
    "    dt = fixed_dates[d+1]\n",
    "    if d+1-n_month<0:\n",
    "        dt_start = fixed_dates[0]\n",
    "    else:\n",
    "        dt_start = fixed_dates[d+1-n_month]\n",
    "            \n",
    "    print('study dataset fot test: date = ',dt,' dt_start = ',dt_start)\n",
    "        \n",
    "    tmp_train  = train_agg.loc[(train_agg.date1<dt) & (train_agg.date1>=dt_start),:] \n",
    "    #а тестовая - на последнюю известную дату\n",
    "    tmp_test  = train_agg.loc[train_agg.date1==last_date,:]   \n",
    "    tmp_right = right_dataset.loc[right_dataset.date1==right_date,:].copy()\n",
    "        \n",
    "    #учиться будем только на study_columns не на всех переменных     \n",
    "    X_train = tmp_train.loc[:,study_columns]\n",
    "    X_test  = tmp_test.loc[:,study_columns]\n",
    "    \n",
    "    print('Максимальная дата обучающей ',tmp_train.date1.max())\n",
    "    print('Миниимальная дата тестовой ',tmp_test.date1.min())   \n",
    "    \n",
    "    y_train = tmp_train[value_column]\n",
    "        \n",
    "    del tmp_train\n",
    "    gc.collect()\n",
    "        \n",
    "    #обучим модель\n",
    "    model.fit(X_train,y_train)\n",
    "        \n",
    "    y_test_pred = model.predict(X_test)\n",
    "    y_test_pred[y_test_pred<0] = 0\n",
    "        \n",
    "\n",
    "\n",
    "    R_test = tmp_test[['bulk_spalen_id','id_flatwork_int']].copy() \n",
    "    R_test['predict'] = y_test_pred \n",
    "             \n",
    "    R_test = R_test.groupby(group_columns) \\\n",
    "                             .agg({'predict':'sum'}) \\\n",
    "                             .reset_index()\n",
    "    tmp_right = tmp_right.merge(R_test, on = group_columns, how = 'left')\n",
    "    submission = tmp_right[['id','bulk_spalen_id','predict']].rename(columns = {'predict':'value'}).fillna(0)\n",
    "    \n",
    "\n",
    "    return submission#, _mse, _grp_mse#, importance, model, full_df_for_calc_cv\n",
    "\n",
    "\n",
    "def my_cv(model, \n",
    "                        dataset, \n",
    "                        right_dataset, \n",
    "                        right_date, \n",
    "                        cv_dates,\n",
    "                        last_date,\n",
    "                        n_month,\n",
    "                        study_columns, \n",
    "                        value_column, \n",
    "                        group_columns, \n",
    "                        random_state=442, \n",
    "                        importance_flag = False):\n",
    "    \n",
    "    #весь обучающий датасет\n",
    "    train_agg = dataset.copy().reset_index(drop = True)\n",
    "    \n",
    "    ind = 0\n",
    "    _mse = np.array([],dtype = 'float')\n",
    "    _grp_mse = np.array([],dtype = 'float')\n",
    "    gc.collect()\n",
    "    print('==========================')\n",
    "    \n",
    "    #основная кросс-валидация\n",
    "    for d in cv_dates:\n",
    "        #получаем даты\n",
    "        dt = fixed_dates[d]\n",
    "        if d-n_month<0:\n",
    "            dt_start = fixed_dates[0]\n",
    "        else:\n",
    "            dt_start = fixed_dates[d-n_month]\n",
    "            \n",
    "        print('ind = ',ind, ' date = ',dt,' dt_start = ',dt_start)\n",
    "        \n",
    "        tmp_train  = train_agg.loc[(train_agg.date1<dt) & (train_agg.date1>=dt_start),:]   \n",
    "        tmp_valid  = train_agg.loc[train_agg.date1==dt,:]\n",
    "        tmp_right = right_dataset.loc[right_dataset.date1==dt,:].copy()\n",
    "        \n",
    "        #учиться будем только на study_columns не на всех переменных     \n",
    "        X_train = tmp_train.loc[:,study_columns]\n",
    "        X_valid = tmp_valid.loc[:,study_columns]\n",
    "        \n",
    "        y_train = tmp_train[value_column]\n",
    "        y_valid = tmp_valid[value_column]\n",
    "        \n",
    "        del tmp_train#, tmp_valid\n",
    "        gc.collect()\n",
    "        \n",
    "        #обучим модель\n",
    "        model.fit(X_train,y_train)\n",
    "        \n",
    "        y_valid_pred = model.predict(X_valid)\n",
    "        y_valid_pred[y_valid_pred<0] = 0\n",
    "        \n",
    "        R_valid = tmp_valid[['bulk_spalen_id','id_flatwork_int']].copy() #X_valid.copy()\n",
    "        R_valid['predict'] = y_valid_pred\n",
    "        \n",
    "        print(f'X_valid.shape = {X_valid.shape:}')\n",
    "        _mse = np.append(_mse,mean_squared_error(y_valid,y_valid_pred))\n",
    "\n",
    "            \n",
    "        R_valid['value_flat'] = y_valid\n",
    "\n",
    "        R_valid = R_valid.groupby(group_columns) \\\n",
    "                             .agg({'predict':'sum','value_flat':'sum'}) \\\n",
    "                             .reset_index()\n",
    "        tmp_right = tmp_right.merge(R_valid, on = group_columns, how = 'left').fillna(0)\n",
    "            \n",
    "        if 1==0:\n",
    "            if ind == 0:\n",
    "                full_df_for_calc_cv = tmp_right[['value','predict']].copy()\n",
    "                full_df_for_calc_cv['ind'] = ind\n",
    "            else:\n",
    "                tmp_df_for_calc_cv = tmp_right[['value','predict']].copy()\n",
    "                tmp_df_for_calc_cv['ind'] = ind\n",
    "                full_df_for_calc_cv = full_df_for_calc_cv.append(tmp_df_for_calc_cv)\n",
    "\n",
    "            _grp_mse = np.append(_grp_mse,mean_squared_error(tmp_right['value'],tmp_right['predict']))\n",
    "\n",
    "        ind = ind + 1\n",
    "\n",
    "        #break\n",
    "        \n",
    "        \n",
    "    #посчитаем важность\n",
    "    importance = pd.DataFrame(dict({'feature':'none', 'delta_mse':0}), index = ['none'])\n",
    "    \n",
    "    mse_now = mean_squared_error(y_valid,y_valid_pred)\n",
    "    NUMBER_SHUFFLE = 5\n",
    "    if importance_flag:\n",
    "        for feature in study_columns:\n",
    "\n",
    "            tmp_mse = 0\n",
    "            for i in range(NUMBER_SHUFFLE):\n",
    "                _X_valid = X_valid.copy()\n",
    "                a = np.asarray(X_valid[feature].copy())\n",
    "                np.random.shuffle(a)\n",
    "                _X_valid[feature] = a\n",
    "                y_valid_pred = model.predict(_X_valid)\n",
    "                tmp_mse = tmp_mse+mean_squared_error(y_valid, y_valid_pred)/NUMBER_SHUFFLE\n",
    "            tmp_importance = pd.DataFrame(dict({'feature':feature, 'delta_mse':(tmp_mse-mse_now)}), index = [feature])    \n",
    "            importance = importance.append(tmp_importance)     \n",
    "    \n",
    "    #Расчитаем для submit-а\n",
    "    \n",
    "    #обучающая сдвигается на 1 месяц вперед\n",
    "    dt = fixed_dates[d+1]\n",
    "    if d+1-n_month<0:\n",
    "        dt_start = fixed_dates[0]\n",
    "    else:\n",
    "        dt_start = fixed_dates[d+1-n_month]\n",
    "            \n",
    "    print('study dataset fot test: date = ',dt,' dt_start = ',dt_start)\n",
    "        \n",
    "    tmp_train  = train_agg.loc[(train_agg.date1<dt) & (train_agg.date1>=dt_start),:] \n",
    "    #а тестовая - на последнюю известную дату\n",
    "    tmp_test  = train_agg.loc[train_agg.date1==last_date,:]   \n",
    "    tmp_right = right_dataset.loc[right_dataset.date1==right_date,:].copy()\n",
    "        \n",
    "    #учиться будем только на study_columns не на всех переменных     \n",
    "    X_train = tmp_train.loc[:,study_columns]\n",
    "    X_test  = tmp_test.loc[:,study_columns]\n",
    "        \n",
    "    y_train = tmp_train[value_column]\n",
    "        \n",
    "    del tmp_train\n",
    "    gc.collect()\n",
    "        \n",
    "    #обучим модель\n",
    "    model.fit(X_train,y_train)\n",
    "        \n",
    "    y_test_pred = model.predict(X_test)\n",
    "    y_test_pred[y_test_pred<0] = 0\n",
    "        \n",
    "    R_test = tmp_test[['bulk_spalen_id','id_flatwork_int']].copy() #X_test.copy()\n",
    "    R_test['predict'] = y_test_pred \n",
    "             \n",
    "    R_test = R_test.groupby(group_columns) \\\n",
    "                             .agg({'predict':'sum'}) \\\n",
    "                             .reset_index()\n",
    "    tmp_right = tmp_right.merge(R_test, on = group_columns, how = 'left')\n",
    "    submission = tmp_right[['id','predict']].rename(columns = {'predict':'value'}).fillna(0)\n",
    "    \n",
    "\n",
    "    return submission, _mse, _grp_mse, importance, model#, full_df_for_calc_cv\n",
    "\n",
    "\n",
    "##############################\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test_days_period = [31 30 31]\n"
     ]
    }
   ],
   "source": [
    "full = prepare_full()\n",
    "flat, dict_bulk_spalen, dict_flat = prepare_flat()\n",
    "\n",
    "full = mem_economy(full)\n",
    "gc.collect()\n",
    "flat = mem_economy(flat)\n",
    "gc.collect()\n",
    "\n",
    "last_date = full[full.is_train==0].date1.dt.date.astype('str').min()\n",
    "\n",
    "#Добавим данные о максимальной площади, доступной для продажи\n",
    "max_sale_square = flat[flat['sale'].astype('str')>last_date].groupby('bulk_spalen_id') \\\n",
    "                                                             .square.sum() \\\n",
    "                                                             .reset_index(name = 'max_square')\n",
    "\n",
    "full = full.merge(max_sale_square, on = 'bulk_spalen_id', how = 'left').fillna(0)\n",
    "\n",
    "\n",
    "\n",
    "status, dict_stat = prepare_status()\n",
    "price             = prepare_price()\n",
    "\n",
    "#status = mem_economy(status)\n",
    "#price  = mem_economy(price)\n",
    "\n",
    "gc.collect()\n",
    "\n",
    "tmp_calendar = full.loc[full['is_train']==0,('date1','date2')].sort_values('date1').drop_duplicates()\n",
    "test_days_period =  np.array(((tmp_calendar.date2 - tmp_calendar.date1)/np.timedelta64(1,'D')+1).astype('int32')) \n",
    "\n",
    "print(f'test_days_period = {test_days_period:}')\n",
    "flat_train = prepare_flat_train(test_days_period)\n",
    "\n",
    "flat_train = mem_economy(flat_train)\n",
    "gc.collect()\n",
    "\n",
    "full['calc_last_value'] = full['calc_last_value'].fillna(0)\n",
    "full = full.reset_index(drop  = True)\n",
    "\n",
    "column_study = np.array(['До метро пешком(км)', 'price', 'mean_sq', 'price_by_square',\n",
    "       'mean_fl', 'Cтавка по ипотеке', 'Станций метро от кольца',\n",
    "       'Площадь двора', 'Date_int', 'До промки(км)', 'month',\n",
    "       'До большой дороги на машине(км)', 'spalen',\n",
    "       'Площадь зеленой зоны в радиусе 500 м', 'bulk_id_int',\n",
    "       'До удобной авторазвязки на машине(км)', 'До парка пешком(км)',\n",
    "       'Курс', 'До Кремля', 'Вклады свыше 3 лет','calc_last_value'])\n",
    "\n",
    "lgb_model = lgb.LGBMRegressor(n_estimators = 150, random_state = 42)\n",
    "\n",
    "for i in range(5):\n",
    "      \n",
    "    submission_lgb, mse, stacking_df, imp_df = my_simple_cv(lgb_model, \n",
    "                                                        full, \n",
    "                                                        column_study, \n",
    "                                                        random_state=442, \n",
    "                                                        importance_flag = True)\n",
    "\n",
    "    submission_lgb = submission_lgb.sort_values('id')\n",
    "    if i==0:\n",
    "        v = submission_lgb['value']\n",
    "    else: \n",
    "        v = v + submission_lgb['value']\n",
    "        \n",
    "submission_lgb['value'] = v/(i+1)   \n",
    "\n",
    "rmse = np.sqrt(mse)\n",
    "\n",
    "\n",
    "submission_lgb = submission_lgb.sort_values('id')\n",
    "filename = f'c1imb3r_lgb_rmse_{(np.mean(rmse)):.4f} +- {(np.std(rmse)):.4f}.csv'\n",
    "submission_lgb.to_csv(filename, index = False)\n",
    "\n",
    "\n",
    "\n",
    "fixed_dates = np.sort(full.date1.dt.strftime('%Y-%m-%d').unique())\n",
    "\n",
    "\n",
    "gc.collect()\n",
    "#определим цену продажи\n",
    "column_filter = ['id_sec','id_gk','id_flatwork','date_settle', \n",
    "                 'date_salestart','sale','bulk_id',\n",
    "                 'date1','realized_1', 'realized_2', 'realized_3',\n",
    "                 'value_1','value_2', 'value_3','dt_to_sale','flat_startsale','date_flat_startsale']\n",
    "\n",
    "                \n",
    "#column_study = np.setdiff1d(np.asarray(flat_train.columns), column_filter)\n",
    "\n",
    "study_cols_1 = np.array(['bulk_spalen_dolya_sqr_stat_new_0',\n",
    "       'bulk_spalen_dolya_sqr_stat_new_1',\n",
    "       'bulk_spalen_dolya_sqr_stat_new_3',\n",
    "       'bulk_spalen_dolya_sqr_stat_new_5', 'bulk_spalen_dolya_stat_new_1',\n",
    "       'bulk_spalen_dolya_stat_new_3', 'bulk_spalen_price_median',\n",
    "       'bulk_spalen_square_mean', 'cnt_stat_new_2', 'cnt_stat_new_3',\n",
    "       'diff_price_max', 'diff_price_min', 'diff_pricem2',\n",
    "       'diff_pricem2_max', 'diff_pricem2_median', 'diff_pricem2_min',\n",
    "       'diff_square_median', 'dt_flat_salestart_delay', 'dt_to_salestart',\n",
    "       'dt_to_settle', 'floor', 'last_stat_new', 'otdelka', 'plan0',\n",
    "       'pricem2_min', 'pricem2_std', 'square', 'stat_new', 'status_days',\n",
    "       'До большой дороги на машине(км)', 'До метро пешком(км)'])\n",
    "\n",
    "study_cols_2 = np.array(['bulk_spalen_dolya_sqr_stat_new_2',\n",
    "       'bulk_spalen_dolya_sqr_stat_new_3',\n",
    "       'bulk_spalen_dolya_sqr_stat_new_5', 'bulk_spalen_dolya_stat_new_2',\n",
    "       'bulk_spalen_dolya_stat_new_3', 'bulk_spalen_dolya_stat_new_5',\n",
    "       'bulk_spalen_price_max', 'bulk_spalen_pricem2_min',\n",
    "       'bulk_spalen_square_std', 'cnt_stat_new', 'cnt_stat_new_2',\n",
    "       'cnt_stat_new_3', 'diff_price_max', 'diff_price_median',\n",
    "       'diff_price_min', 'diff_pricem2_max', 'diff_pricem2_median',\n",
    "       'diff_pricem2_min', 'diff_square_min', 'dt_flat_salestart_delay',\n",
    "       'dt_settle_salestart', 'dt_to_salestart', 'dt_to_settle', 'floor',\n",
    "       'last_stat_new', 'otdelka', 'plan0', 'pricem2_min', 'sqr_stat_new',\n",
    "       'square', 'stage_number', 'stat_new', 'status_days',\n",
    "       'was_decrease_mean', 'До большой дороги на машине(км)',\n",
    "       'До метро пешком(км)', 'До парка пешком(км)',\n",
    "       'Количество помещений'])\n",
    "\n",
    "study_cols_3 = np.array(['bulk_spalen_dolya_sqr_stat_new_1',\n",
    "       'bulk_spalen_dolya_sqr_stat_new_2',\n",
    "       'bulk_spalen_dolya_sqr_stat_new_3',\n",
    "       'bulk_spalen_dolya_sqr_stat_new_5', 'bulk_spalen_dolya_stat_new_0',\n",
    "       'bulk_spalen_dolya_stat_new_1', 'bulk_spalen_dolya_stat_new_3',\n",
    "       'bulk_spalen_dolya_stat_new_5', 'bulk_spalen_pricem2_median',\n",
    "       'bulk_spalen_square_median', 'bulk_spalen_square_min',\n",
    "       'cnt_stat_new', 'cnt_stat_new_0', 'cnt_stat_new_1',\n",
    "       'cnt_stat_new_2', 'diff_pricem2', 'diff_pricem2_max',\n",
    "       'diff_pricem2_min', 'dt_flat_salestart_delay', 'dt_to_salestart',\n",
    "       'floor', 'otdelka', 'price', 'pricem2', 'pricem2_min',\n",
    "       'sqr_stat_new', 'stat_new', 'status_days', 'was_decrease_mean',\n",
    "       'Машиномест'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==========================\n",
      "study dataset fot test: date =  2018-03-01  dt_start =  2016-12-01\n",
      "Максимальная дата обучающей  2018-02-01\n",
      "Миниимальная дата тестовой  2018-03-01\n"
     ]
    }
   ],
   "source": [
    "n_month = 15\n",
    "lgb_model = lgb.LGBMRegressor(n_estimators = 500, \n",
    "                                  max_depth = 5, \n",
    "                                  learning_rate = 0.05, \n",
    "                                  random_state = 42+i, predict_leaf_index = True)\n",
    "\n",
    "submission_1 = my_submit(\n",
    "                            model = lgb_model, \n",
    "                            dataset = flat_train,\n",
    "                            right_dataset = full[['id','is_train','bulk_spalen_id','value','date1']],\n",
    "                            right_date = fixed_dates[-3], \n",
    "                            cv_dates = [len(fixed_dates)-4], \n",
    "                            last_date = last_date,\n",
    "                            n_month = n_month,\n",
    "                            study_columns = study_cols_1, \n",
    "                            value_column = 'value_1', \n",
    "                            group_columns = 'bulk_spalen_id',\n",
    "                            random_state=442, \n",
    "                            importance_flag = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>bulk_spalen_id</th>\n",
       "      <th>value</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1072</td>\n",
       "      <td>467</td>\n",
       "      <td>244.107913</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1386</td>\n",
       "      <td>589</td>\n",
       "      <td>216.133797</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1389</td>\n",
       "      <td>641</td>\n",
       "      <td>1.097742</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1388</td>\n",
       "      <td>640</td>\n",
       "      <td>149.243550</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1402</td>\n",
       "      <td>435</td>\n",
       "      <td>202.864564</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1387</td>\n",
       "      <td>587</td>\n",
       "      <td>104.309866</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1385</td>\n",
       "      <td>586</td>\n",
       "      <td>56.776716</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1068</td>\n",
       "      <td>325</td>\n",
       "      <td>164.316202</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>1067</td>\n",
       "      <td>326</td>\n",
       "      <td>648.971505</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>1069</td>\n",
       "      <td>327</td>\n",
       "      <td>191.353653</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>1070</td>\n",
       "      <td>328</td>\n",
       "      <td>54.231076</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>1073</td>\n",
       "      <td>465</td>\n",
       "      <td>175.077891</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>1403</td>\n",
       "      <td>433</td>\n",
       "      <td>491.317811</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>1071</td>\n",
       "      <td>466</td>\n",
       "      <td>756.163775</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>1384</td>\n",
       "      <td>588</td>\n",
       "      <td>473.933747</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>1404</td>\n",
       "      <td>434</td>\n",
       "      <td>653.727013</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>408</td>\n",
       "      <td>811</td>\n",
       "      <td>110.249376</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>1406</td>\n",
       "      <td>402</td>\n",
       "      <td>614.320252</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>403</td>\n",
       "      <td>677</td>\n",
       "      <td>52.430260</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>402</td>\n",
       "      <td>679</td>\n",
       "      <td>266.876073</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>401</td>\n",
       "      <td>678</td>\n",
       "      <td>627.931373</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>1074</td>\n",
       "      <td>172</td>\n",
       "      <td>342.978656</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>404</td>\n",
       "      <td>810</td>\n",
       "      <td>150.378842</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>405</td>\n",
       "      <td>808</td>\n",
       "      <td>280.617480</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>406</td>\n",
       "      <td>807</td>\n",
       "      <td>121.638003</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>1405</td>\n",
       "      <td>432</td>\n",
       "      <td>72.481547</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>407</td>\n",
       "      <td>809</td>\n",
       "      <td>849.711800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>410</td>\n",
       "      <td>504</td>\n",
       "      <td>871.331182</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>411</td>\n",
       "      <td>503</td>\n",
       "      <td>800.004395</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>412</td>\n",
       "      <td>505</td>\n",
       "      <td>381.138011</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>555</th>\n",
       "      <td>171</td>\n",
       "      <td>33</td>\n",
       "      <td>857.050349</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>556</th>\n",
       "      <td>1537</td>\n",
       "      <td>937</td>\n",
       "      <td>111.456845</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>557</th>\n",
       "      <td>1536</td>\n",
       "      <td>317</td>\n",
       "      <td>31.428645</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>558</th>\n",
       "      <td>1548</td>\n",
       "      <td>638</td>\n",
       "      <td>1457.621640</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>559</th>\n",
       "      <td>202</td>\n",
       "      <td>730</td>\n",
       "      <td>152.831224</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>560</th>\n",
       "      <td>1546</td>\n",
       "      <td>583</td>\n",
       "      <td>606.498768</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>561</th>\n",
       "      <td>208</td>\n",
       "      <td>760</td>\n",
       "      <td>70.826118</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>562</th>\n",
       "      <td>1019</td>\n",
       "      <td>63</td>\n",
       "      <td>126.978787</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>563</th>\n",
       "      <td>1018</td>\n",
       "      <td>60</td>\n",
       "      <td>443.136322</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>564</th>\n",
       "      <td>1017</td>\n",
       "      <td>62</td>\n",
       "      <td>227.570072</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>565</th>\n",
       "      <td>1016</td>\n",
       "      <td>98</td>\n",
       "      <td>809.237003</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>566</th>\n",
       "      <td>1015</td>\n",
       "      <td>100</td>\n",
       "      <td>35.456766</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>567</th>\n",
       "      <td>210</td>\n",
       "      <td>257</td>\n",
       "      <td>15.161171</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>568</th>\n",
       "      <td>209</td>\n",
       "      <td>769</td>\n",
       "      <td>8.369515</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>569</th>\n",
       "      <td>203</td>\n",
       "      <td>731</td>\n",
       "      <td>64.510390</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>570</th>\n",
       "      <td>1022</td>\n",
       "      <td>742</td>\n",
       "      <td>168.988489</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>571</th>\n",
       "      <td>1023</td>\n",
       "      <td>741</td>\n",
       "      <td>102.378237</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>572</th>\n",
       "      <td>1024</td>\n",
       "      <td>816</td>\n",
       "      <td>174.929893</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>573</th>\n",
       "      <td>1020</td>\n",
       "      <td>61</td>\n",
       "      <td>437.062991</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>574</th>\n",
       "      <td>1021</td>\n",
       "      <td>743</td>\n",
       "      <td>118.348732</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>575</th>\n",
       "      <td>1550</td>\n",
       "      <td>637</td>\n",
       "      <td>33.359428</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>576</th>\n",
       "      <td>1554</td>\n",
       "      <td>164</td>\n",
       "      <td>205.775494</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>577</th>\n",
       "      <td>204</td>\n",
       "      <td>735</td>\n",
       "      <td>287.367777</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>578</th>\n",
       "      <td>205</td>\n",
       "      <td>734</td>\n",
       "      <td>133.401680</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>579</th>\n",
       "      <td>1553</td>\n",
       "      <td>165</td>\n",
       "      <td>186.374966</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>580</th>\n",
       "      <td>206</td>\n",
       "      <td>738</td>\n",
       "      <td>138.418328</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>581</th>\n",
       "      <td>1552</td>\n",
       "      <td>162</td>\n",
       "      <td>72.900481</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>582</th>\n",
       "      <td>1551</td>\n",
       "      <td>639</td>\n",
       "      <td>129.001701</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>583</th>\n",
       "      <td>207</td>\n",
       "      <td>759</td>\n",
       "      <td>31.971149</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>584</th>\n",
       "      <td>1549</td>\n",
       "      <td>636</td>\n",
       "      <td>97.271719</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>585 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       id  bulk_spalen_id        value\n",
       "0    1072             467   244.107913\n",
       "1    1386             589   216.133797\n",
       "2    1389             641     1.097742\n",
       "3    1388             640   149.243550\n",
       "4    1402             435   202.864564\n",
       "5    1387             587   104.309866\n",
       "6    1385             586    56.776716\n",
       "7    1068             325   164.316202\n",
       "8    1067             326   648.971505\n",
       "9    1069             327   191.353653\n",
       "10   1070             328    54.231076\n",
       "11   1073             465   175.077891\n",
       "12   1403             433   491.317811\n",
       "13   1071             466   756.163775\n",
       "14   1384             588   473.933747\n",
       "15   1404             434   653.727013\n",
       "16    408             811   110.249376\n",
       "17   1406             402   614.320252\n",
       "18    403             677    52.430260\n",
       "19    402             679   266.876073\n",
       "20    401             678   627.931373\n",
       "21   1074             172   342.978656\n",
       "22    404             810   150.378842\n",
       "23    405             808   280.617480\n",
       "24    406             807   121.638003\n",
       "25   1405             432    72.481547\n",
       "26    407             809   849.711800\n",
       "27    410             504   871.331182\n",
       "28    411             503   800.004395\n",
       "29    412             505   381.138011\n",
       "..    ...             ...          ...\n",
       "555   171              33   857.050349\n",
       "556  1537             937   111.456845\n",
       "557  1536             317    31.428645\n",
       "558  1548             638  1457.621640\n",
       "559   202             730   152.831224\n",
       "560  1546             583   606.498768\n",
       "561   208             760    70.826118\n",
       "562  1019              63   126.978787\n",
       "563  1018              60   443.136322\n",
       "564  1017              62   227.570072\n",
       "565  1016              98   809.237003\n",
       "566  1015             100    35.456766\n",
       "567   210             257    15.161171\n",
       "568   209             769     8.369515\n",
       "569   203             731    64.510390\n",
       "570  1022             742   168.988489\n",
       "571  1023             741   102.378237\n",
       "572  1024             816   174.929893\n",
       "573  1020              61   437.062991\n",
       "574  1021             743   118.348732\n",
       "575  1550             637    33.359428\n",
       "576  1554             164   205.775494\n",
       "577   204             735   287.367777\n",
       "578   205             734   133.401680\n",
       "579  1553             165   186.374966\n",
       "580   206             738   138.418328\n",
       "581  1552             162    72.900481\n",
       "582  1551             639   129.001701\n",
       "583   207             759    31.971149\n",
       "584  1549             636    97.271719\n",
       "\n",
       "[585 rows x 3 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==========================\n",
      "study dataset fot test: date =  2018-03-01  dt_start =  2016-12-01\n",
      "Максимальная дата обучающей  2018-02-01\n",
      "Миниимальная дата тестовой  2018-03-01\n",
      "S1\n",
      "==========================\n",
      "study dataset fot test: date =  2018-02-01  dt_start =  2016-11-01\n",
      "Максимальная дата обучающей  2018-01-01\n",
      "Миниимальная дата тестовой  2018-03-01\n",
      "S2\n",
      "==========================\n",
      "study dataset fot test: date =  2018-01-01  dt_start =  2016-10-01\n",
      "Максимальная дата обучающей  2017-12-01\n",
      "Миниимальная дата тестовой  2018-03-01\n",
      "S3\n",
      "==========================\n",
      "study dataset fot test: date =  2018-03-01  dt_start =  2016-12-01\n",
      "Максимальная дата обучающей  2018-02-01\n",
      "Миниимальная дата тестовой  2018-03-01\n",
      "S1\n",
      "==========================\n",
      "study dataset fot test: date =  2018-02-01  dt_start =  2016-11-01\n",
      "Максимальная дата обучающей  2018-01-01\n",
      "Миниимальная дата тестовой  2018-03-01\n",
      "S2\n",
      "==========================\n",
      "study dataset fot test: date =  2018-01-01  dt_start =  2016-10-01\n",
      "Максимальная дата обучающей  2017-12-01\n",
      "Миниимальная дата тестовой  2018-03-01\n",
      "S3\n",
      "==========================\n",
      "study dataset fot test: date =  2018-03-01  dt_start =  2016-12-01\n",
      "Максимальная дата обучающей  2018-02-01\n",
      "Миниимальная дата тестовой  2018-03-01\n",
      "S1\n",
      "==========================\n",
      "study dataset fot test: date =  2018-02-01  dt_start =  2016-11-01\n",
      "Максимальная дата обучающей  2018-01-01\n",
      "Миниимальная дата тестовой  2018-03-01\n",
      "S2\n",
      "==========================\n",
      "study dataset fot test: date =  2018-01-01  dt_start =  2016-10-01\n",
      "Максимальная дата обучающей  2017-12-01\n",
      "Миниимальная дата тестовой  2018-03-01\n",
      "S3\n",
      "==========================\n",
      "study dataset fot test: date =  2018-03-01  dt_start =  2016-12-01\n",
      "Максимальная дата обучающей  2018-02-01\n",
      "Миниимальная дата тестовой  2018-03-01\n",
      "S1\n",
      "==========================\n",
      "study dataset fot test: date =  2018-02-01  dt_start =  2016-11-01\n",
      "Максимальная дата обучающей  2018-01-01\n",
      "Миниимальная дата тестовой  2018-03-01\n",
      "S2\n",
      "==========================\n",
      "study dataset fot test: date =  2018-01-01  dt_start =  2016-10-01\n",
      "Максимальная дата обучающей  2017-12-01\n",
      "Миниимальная дата тестовой  2018-03-01\n",
      "S3\n",
      "==========================\n",
      "study dataset fot test: date =  2018-03-01  dt_start =  2016-12-01\n",
      "Максимальная дата обучающей  2018-02-01\n",
      "Миниимальная дата тестовой  2018-03-01\n",
      "S1\n",
      "==========================\n",
      "study dataset fot test: date =  2018-02-01  dt_start =  2016-11-01\n",
      "Максимальная дата обучающей  2018-01-01\n",
      "Миниимальная дата тестовой  2018-03-01\n",
      "S2\n",
      "==========================\n",
      "study dataset fot test: date =  2018-01-01  dt_start =  2016-10-01\n",
      "Максимальная дата обучающей  2017-12-01\n",
      "Миниимальная дата тестовой  2018-03-01\n",
      "S3\n",
      "(1817, 3)\n",
      "(1817, 3)\n"
     ]
    }
   ],
   "source": [
    "last_date = full[full.is_train==0].date1.dt.date.astype('str').min()\n",
    "\n",
    "for i in range(5):\n",
    "    n_month = 15\n",
    "    lgb_model = lgb.LGBMRegressor(n_estimators = 500, \n",
    "                                  max_depth = 5, \n",
    "                                  learning_rate = 0.05, \n",
    "                                  random_state = 42+i, predict_leaf_index = True)\n",
    "    \n",
    "    submission_1 = my_submit(\n",
    "                            model = lgb_model, \n",
    "                            dataset = flat_train,\n",
    "                            right_dataset = full[['id','is_train','bulk_spalen_id','value','date1']],\n",
    "                            right_date = fixed_dates[-3], \n",
    "                            cv_dates = [len(fixed_dates)-4], \n",
    "                            last_date = last_date,\n",
    "                            n_month = n_month,\n",
    "                            study_columns = study_cols_1, \n",
    "                            value_column = 'value_1', \n",
    "                            group_columns = 'bulk_spalen_id',\n",
    "                            random_state=442, \n",
    "                            importance_flag = True)\n",
    "    print('S1')\n",
    "\n",
    "    submission_2 = my_submit(\n",
    "                            model = lgb_model, \n",
    "                            dataset = flat_train,\n",
    "                            right_dataset = full[['id','is_train','bulk_spalen_id','value','date1']],\n",
    "                            right_date = fixed_dates[-2], \n",
    "                            cv_dates = [len(fixed_dates)-5],\n",
    "                            last_date = last_date,\n",
    "                            n_month = n_month,\n",
    "                            study_columns = study_cols_2, \n",
    "                            value_column = 'value_2', \n",
    "                            group_columns = 'bulk_spalen_id',\n",
    "                            random_state=442, \n",
    "                            importance_flag = True)\n",
    "\n",
    "    print('S2')\n",
    "    submission_3 = my_submit(\n",
    "                            model = lgb_model, \n",
    "                            dataset = flat_train,\n",
    "                            right_dataset = full[['id','is_train','bulk_spalen_id','value','date1']],\n",
    "                            right_date = fixed_dates[-1],\n",
    "                            cv_dates = [len(fixed_dates)-6],\n",
    "                            last_date = last_date,\n",
    "                            n_month = n_month,\n",
    "                            study_columns = study_cols_3, \n",
    "                            value_column = 'value_3', \n",
    "                            group_columns = 'bulk_spalen_id',\n",
    "                            random_state=442, \n",
    "                            importance_flag = True)\n",
    "\n",
    "\n",
    "    print('S3')\n",
    "    submission_flat = pd.concat([submission_1,submission_2,submission_3]).fillna(0).sort_values('id')\n",
    "    \n",
    "    if i==0:\n",
    "        v = submission_flat['value']\n",
    "    else: \n",
    "        v = v + submission_flat['value']\n",
    "        \n",
    "submission_flat['value'] = v/(i+1)  \n",
    "submission_flat = submission_flat.sort_values('id').reset_index(drop = True)\n",
    "\n",
    "filename = f'c1imb3r_x_{(i+1):}_nmonth_15.csv'\n",
    "submission_flat.to_csv(filename, index = False)\n",
    "\n",
    "\n",
    "best_lgb = submission_lgb.rename(columns = {'value':'predict'})\n",
    "best_flat = submission_flat.rename(columns = {'value':'predict'})\n",
    "\n",
    "print(best_lgb.shape)\n",
    "print(best_flat.shape)\n",
    "\n",
    "best = best_flat.merge(best_lgb, on = ['id','bulk_spalen_id'], how = 'left')\n",
    "mic_c = 0.7\n",
    "\n",
    "best['predict'] = best['predict_x']\n",
    "best.loc[best['predict_x']==0, 'predict'] = best.loc[best['predict_x']==0, 'predict_y']\n",
    "best['predict'] = mic_c*best['predict']+(1-mic_c)*best['predict_y']\n",
    "     \n",
    "best['value'] = best['predict'] \n",
    "\n",
    "filename = f'c1imb3r_submit.csv'\n",
    "best[['id','value']].to_csv(filename, index = False)\n",
    "\n",
    "\n",
    "#Добавим знания о квартирах\n",
    "max_sale_square = flat[(flat['sale'].astype('str')=='2020-01-02') & \n",
    "                      (flat['flat_startsale'] < '2018-06-02')].groupby('bulk_spalen_id') \\\n",
    "                                   .agg({'square':'sum'}) \\\n",
    "                                   .reset_index()\n",
    "res_sale_square = best.groupby('bulk_spalen_id').agg({'predict':'sum'}).reset_index()\n",
    "\n",
    "res_sale_square = res_sale_square.merge(max_sale_square, on = 'bulk_spalen_id', how = 'left').fillna(0)\n",
    "res_sale_square['coeff'] = res_sale_square['square']/res_sale_square['predict']\n",
    "res_sale_square.loc[res_sale_square['coeff']>1, 'coeff'] = 1\n",
    "\n",
    "\n",
    "best = best.merge(res_sale_square[['bulk_spalen_id','coeff']], how = 'left', on = 'bulk_spalen_id')\n",
    "best['predict'] = best['predict']*best['coeff']\n",
    "\n",
    "best['value'] = best['predict'] \n",
    "\n",
    "filename = f'c1imb3r_submit_max.csv'\n",
    "best[['id','value']].to_csv(filename, index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
