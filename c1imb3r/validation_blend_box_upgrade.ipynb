{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sys\n",
    "import gc\n",
    "sys.path.append('/usr/local/lib/python3.6/site-packages')\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from datetime import datetime\n",
    "\n",
    "from sklearn.model_selection import train_test_split, KFold, StratifiedKFold, RepeatedStratifiedKFold\n",
    "from sklearn.metrics import roc_auc_score, mean_squared_error, make_scorer\n",
    "from sklearn.linear_model import LogisticRegression, LogisticRegressionCV\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "\n",
    "import lightgbm as lgb\n",
    "#from hyperopt import hp, tpe\n",
    "#from hyperopt.fmin import fmin\n",
    "\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip3 install -U hyperopt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def mem_economy(dataset):\n",
    "    features = np.array(dataset.dtypes[dataset.dtypes=='float64'].index)\n",
    "    for f in features:\n",
    "        dataset[f] = dataset[f].astype('float32')\n",
    "    features = np.array(dataset.dtypes[dataset.dtypes=='int64'].index)\n",
    "    for f in features:\n",
    "        dataset[f] = dataset[f].astype('int32')\n",
    "    \n",
    "    return dataset\n",
    "\n",
    "def my_drop_levels(dataset, sep = '_', brief = ''):\n",
    "    if dataset.columns.nlevels>1:\n",
    "        new_columns = np.array([], dtype = 'str')\n",
    "        for col_i in range(dataset.shape[1]):\n",
    "            col_name = brief\n",
    "            for level in range(dataset.columns.nlevels):\n",
    "                tmp_col_name = dataset.columns.levels[level][dataset.columns.labels[level][col_i]]#.astype('str')\n",
    "                tmp_col_name = str(tmp_col_name) \n",
    "                if (level>0) & (tmp_col_name!=''):\n",
    "                    col_name = col_name+'_'\n",
    "                col_name = col_name+tmp_col_name\n",
    "            new_columns = np.append(new_columns,col_name)\n",
    "            #print(col_name)\n",
    "        for level in range(dataset.columns.nlevels-1):\n",
    "            dataset.columns.droplevel(0)\n",
    "        dataset.columns = new_columns      \n",
    "    return dataset\n",
    "\n",
    "def prepare_full():\n",
    "    train = pd.read_csv('../files-pik_digital_day/train.csv', encoding='cp1251')\n",
    "    test = pd.read_csv('../files-pik_digital_day/test.csv', encoding='cp1251')\n",
    "    \n",
    "    train['is_train'] = 1\n",
    "    test['is_train'] = 0\n",
    "    \n",
    "    full = pd.concat([train,test])\n",
    "    \n",
    "    del train, test\n",
    "    gc.collect()\n",
    "    \n",
    "    full = full.sort_values('month_cnt')\n",
    "    \n",
    "    le = LabelEncoder()\n",
    "    full['bulk_id_int'] = le.fit_transform(full['bulk_id'])\n",
    "    full['date1'] = pd.to_datetime(full['date1'], format='%Y-%m-%d')\n",
    "    GLOBAL_MINDATE = full['date1'].min()\n",
    "    full['Date_int'] = ((full['date1'] - GLOBAL_MINDATE)/np.timedelta64(1, 'D')).astype('int32')\n",
    "\n",
    "    full['Автомойка'] = (full['Автомойка']=='да').astype('int')\n",
    "    full['Входные группы'] = (full['Входные группы']=='да').astype('int')\n",
    "    full['Двор без машин'] = (full['Двор без машин']=='да').astype('int')\n",
    "    full['Класс объекта'] = full['Класс объекта'].map({'эконом':1, 'комфорт':3, 'стандарт':2})\n",
    "    full['Кладовые'] = (full['Кладовые']=='да').astype('int')\n",
    "    full['Колясочные'] = (full['Колясочные']=='да').astype('int')\n",
    "    full['Огорожена территория'] = (full['Огорожена территория']=='да').astype('int')\n",
    "    full['Подземная парковка'] = (full['Подземная парковка']=='да').astype('int')\n",
    "    full['Система мусоротведения'] = le.fit_transform(full['Система мусоротведения'])\n",
    "    full['Спортивная площадка'] = (full['Спортивная площадка']=='да').astype('int')\n",
    "    \n",
    "    #введем уникальные id\n",
    "    full['bulk_spalen_id'] = full['bulk_id_int'].astype('str')+'_'+full['spalen'].astype('str')\n",
    "    full['bulk_spalen_id'] = le.fit_transform(full['bulk_spalen_id'])\n",
    "     \n",
    "    # подсчитаем псевдо start_square (без учета возвращенных)\n",
    "    full['calc_start_square'] = full.groupby(['bulk_spalen_id'])['start_square'].shift(1) - full.groupby(['bulk_spalen_id'])['value'].shift(1)\n",
    "    full['calc_last_value'] = full.groupby(['bulk_spalen_id'])['value'].shift(1)\n",
    "    \n",
    "    full['date2'] = full.date1+ pd.offsets.MonthEnd(1)\n",
    "    \n",
    "    full['price_by_square'] = full['price']/full['mean_sq']\n",
    "    \n",
    "    \n",
    "    full = full.reset_index(drop = True)\n",
    "    \n",
    "    return full\n",
    "    \n",
    "\n",
    "def prepare_flat():\n",
    "    le = LabelEncoder()\n",
    "    flat = pd.read_csv('../files-pik_digital_day/flat.csv', encoding='cp1251')\n",
    "    flat = flat.rename(columns = {'id_bulk':'bulk_id'})\n",
    "    flat['id_flatwork_int'] = np.array(flat.index).astype('int')\n",
    "    \n",
    "    dict_bulk_spalen = full.loc[:, ('bulk_id','bulk_id_int','spalen','bulk_spalen_id')] \\\n",
    "                       .drop_duplicates() \n",
    "    dict_flat = flat[['id_flatwork_int','id_flatwork','bulk_id','spalen']].copy()\n",
    "\n",
    "    dict_flat = dict_flat.merge(dict_bulk_spalen, how = 'left')\n",
    "    \n",
    "    \n",
    "    flat['Автомойка'] = (flat['Автомойка']=='да').astype('int')\n",
    "    flat['Входные группы'] = (flat['Входные группы']=='да').astype('int')\n",
    "    flat['Двор без машин'] = (flat['Двор без машин']=='да').astype('int')\n",
    "    \n",
    "\n",
    "    flat['Класс объекта'] = flat['Класс объекта'].fillna('эконом').map({'эконом':1, 'комфорт':3, 'стандарт':2}).astype('int')\n",
    "    flat['Кладовые'] = (flat['Кладовые']=='да').astype('int')\n",
    "    flat['Колясочные'] = (flat['Колясочные']=='да').astype('int')\n",
    "    flat['Огорожена территория'] = (flat['Огорожена территория']=='да').astype('int')\n",
    "    flat['Подземная парковка'] = (flat['Подземная парковка']=='да').astype('int')\n",
    "    flat.drop('Система мусоротведения', axis = 1, inplace = True)\n",
    "    #flat['Система мусоротведения'] = le.fit_transform(flat['Система мусоротведения'])\n",
    "    flat['Спортивная площадка'] = (flat['Спортивная площадка']=='да').astype('int')\n",
    "    flat['otdelka'] = le.fit_transform(flat['otdelka'].fillna('nan'))\n",
    "    flat['vid'] = flat['vid'].map({'эконом':1, 'средний':2, 'хороший':3}).fillna(0).astype('int')\n",
    "    flat['plan_size'] = flat['plan_size'].fillna('-1').map({'S':1, 'M':2, 'L':3, '-1':0}).astype('int')\n",
    "    flat['plan0'] = le.fit_transform(flat['plan0'].fillna('nan'))\n",
    "    \n",
    "\n",
    "    flat['date_flat_startsale'] = pd.to_datetime(flat['flat_startsale'].fillna('2018-03-01'), format='%Y-%m-%d')\n",
    "    flat['date_settle'] = pd.to_datetime(flat['date_settle'], format='%Y-%m-%d')\n",
    "    flat['date_salestart'] = pd.to_datetime(flat['date_salestart'].fillna('2018-03-01'), format='%Y-%m-%d')\n",
    "    \n",
    "    flat['dt_flat_salestart_delay'] = ((flat['date_flat_startsale'] - flat['date_salestart'])/np.timedelta64(1, 'D')).astype('int32') \n",
    "    \n",
    "    \n",
    "    flat['sale'] = (pd.to_datetime(flat['sale'], format = '%Y-%m-%d %H:%M:%S') +  np.timedelta64(1,'D') \n",
    "                   ).dt.date\n",
    "\n",
    "\n",
    "    flat.loc[~flat['date_settle'].isna(),'dt_settle_salestart'] = ((flat.loc[~flat['date_settle'].isna(),'date_settle'] - flat.loc[~flat['date_settle'].isna(),'date_salestart'])/np.timedelta64(1, 'D')).astype('int32')\n",
    "\n",
    "    #заполним медианным значением\n",
    "    dt_settle_salestart_median = int(flat.loc[~flat['date_settle'].isna(),'dt_settle_salestart'].median())\n",
    "    flat.loc[flat['date_settle'].isna(),'dt_settle_salestart'] = dt_settle_salestart_median  \n",
    "    \n",
    "    flat['dt_settle_salestart'] = flat['dt_settle_salestart'].astype('int32')\n",
    "    \n",
    "    flat.loc[flat['date_settle'].isna(),'date_settle'] = flat.loc[flat['date_settle'].isna(),'date_salestart'] + np.timedelta64(dt_settle_salestart_median, 'D')\n",
    "    \n",
    "    \n",
    "    flat = flat.merge(dict_flat[['id_flatwork_int','bulk_spalen_id','bulk_id_int']], how = 'left', on = 'id_flatwork_int')\n",
    "     \n",
    "    return flat, dict_bulk_spalen, dict_flat\n",
    "    \n",
    "def prepare_status():\n",
    "    \n",
    "\n",
    "\n",
    "    status = pd.read_csv('../files-pik_digital_day/status.csv', encoding='cp1251')\n",
    "    status = status.merge(dict_flat, how = 'inner')\n",
    "\n",
    "    #удалим статусы-однодневки\n",
    "    status = status[status['datefrom']!=status['dateto']]\n",
    "\n",
    "    dict_stat = status[['stat','stat_name']].drop_duplicates()\n",
    "    dict_stat['can_be_sold'] = (~dict_stat.stat_name.isin(['Реализован','Статус после покупки'])).astype('int16')\n",
    "    dict_stat['sold'] = (dict_stat.stat_name.isin(['Реализован','Статус после покупки'])).astype('int16')\n",
    "    dict_stat['realize'] = (dict_stat.stat_name.isin(['Реализован'])).astype('int16')\n",
    "    dict_stat['stat_new'] = dict_stat['stat_name'].map({'Не реализуется':0,\n",
    "                                                        'В реализации (не на сайте)':1,\n",
    "                                                        'В реализации':2,\n",
    "                                                        'Онлайн бронирование':3,\n",
    "                                                        'Зарезервирован под клиента':3,\n",
    "                                                        'Платное бронирование':4,\n",
    "                                                        'Реализован':5,\n",
    "                                                        'Статус после покупки':6\n",
    "                                                        }).astype('int16')\n",
    "\n",
    "    status = status.merge(dict_stat[['stat','stat_new']], how = 'inner')\n",
    "\n",
    "\n",
    "    status['datefrom'] = (pd.to_datetime(status['datefrom'], format = '%Y-%m-%d %H:%M:%S') + np.timedelta64(1,'D')).dt.date.astype('str')\n",
    "    status['dateto'] = (pd.to_datetime(status['dateto'], format = '%Y-%m-%d %H:%M:%S') + np.timedelta64(1,'D')).dt.date.astype('str')\n",
    "    \n",
    "    #удаляем всё, что не подходит для валидации\n",
    "    status = status[status['datefrom']<=valid_date]\n",
    "    status.loc[(status['dateto']>valid_date), 'dateto'] = '2100-01-02'\n",
    "\n",
    "    status['datefrom_dt'] = pd.to_datetime(status['datefrom'], format='%Y-%m-%d')\n",
    "    status['dateto_dt'] = pd.to_datetime(status['dateto'], format='%Y-%m-%d')\n",
    "\n",
    "\n",
    "    status = status.sort_values(['datefrom','dateto'])\n",
    "    status['last_stat_new'] = status.groupby(['id_flatwork_int'])['stat_new'].shift(1)#.fillna('stat_new')\n",
    "    status.loc[status['last_stat_new'].isna(),'last_stat_new'] = -1\n",
    "\n",
    "    \n",
    "\n",
    "    status['delay'] = (status['dateto_dt']-status['datefrom_dt'])/np.timedelta64(1,'D')\n",
    "\n",
    "\n",
    "    gc.collect() \n",
    "    \n",
    "    return status, dict_stat\n",
    "    \n",
    "def prepare_price():\n",
    "    \n",
    "    price = pd.read_csv('../files-pik_digital_day/price.csv', encoding='utf-8')\n",
    "    price = price.merge(dict_flat, how = 'inner')\n",
    "\n",
    "    #удалим пустые цены и цены однодневки\n",
    "    price = price[(price.pricem2>1) & (price['datefrom']!=price['dateto'])].sort_values(['datefrom','dateto'])\n",
    "\n",
    "    price['datefrom'] = (pd.to_datetime(price['datefrom'], format = '%Y-%m-%d %H:%M:%S') + np.timedelta64(1,'D')).dt.date.astype('str')\n",
    "    price['dateto'] = (pd.to_datetime(price['dateto'], format = '%Y-%m-%d %H:%M:%S') + np.timedelta64(1,'D')).dt.date.astype('str')\n",
    "    \n",
    "    #удаляем всё, что не подходит для валидации\n",
    "    price = price[price['datefrom']<=valid_date]\n",
    "    price.loc[(price['dateto']>valid_date), 'dateto'] = '2100-01-02'\n",
    "\n",
    "\n",
    "    price['last_pricem2'] = price.groupby(['id_flatwork_int'])['pricem2'].shift(1).fillna(0)\n",
    "    price['diff_pricem2'] = price['pricem2'] - price['last_pricem2']\n",
    "    price['was_decrease'] = (price['diff_pricem2'] < 0).astype('int32')\n",
    "\n",
    "\n",
    "\n",
    "    price['datefrom_dt'] = pd.to_datetime(price['datefrom'], format='%Y-%m-%d')\n",
    "    price['dateto_dt'] = pd.to_datetime(price['dateto'], format='%Y-%m-%d')\n",
    "\n",
    "    price = price.merge(flat.loc[:,('sale','id_flatwork_int')], how = 'left')\n",
    "    price['sale'] = pd.to_datetime(price['sale'], format='%Y-%m-%d')\n",
    "    price['is_saled_price'] = ((price['sale']>=price['datefrom_dt']) & (price['sale']<price['dateto_dt'])).astype('int')\n",
    "\n",
    "    price['delay'] = (price['dateto_dt']-price['datefrom_dt'])/np.timedelta64(1,'D')\n",
    "\n",
    "    gc.collect()\n",
    "    \n",
    "    return price   \n",
    "\n",
    "def prepare_flat_train(test_days_period):\n",
    "    \n",
    "\n",
    "    fixed_dates = np.sort(full.date1.dt.strftime('%Y-%m-%d').unique())\n",
    "    fixed_dates_last = np.sort(full.date2.dt.strftime('%Y-%m-%d').unique())\n",
    "\n",
    "\n",
    "    for i in range(len(fixed_dates)):\n",
    "        gc.collect()\n",
    "\n",
    "\n",
    "        fixed_date = fixed_dates[i] \n",
    "        fixed_date_last = fixed_dates_last[i]\n",
    "\n",
    "\n",
    "        #найдем квартиры, доступные к продаже на эту дату\n",
    "        status_on_date = status[(status.datefrom<=fixed_date) & (status.dateto>fixed_date)].copy()\n",
    "\n",
    "\n",
    "        #срок жизни статуса\n",
    "        status_on_date['datefrom'] = pd.to_datetime(status_on_date['datefrom'], format='%Y-%m-%d')\n",
    "        status_on_date['datenow'] = pd.to_datetime(fixed_date, format='%Y-%m-%d')\n",
    "        #status_on_date['dateto'] = pd.to_datetime(status_on_date['dateto'], format='%Y-%m-%d %H:%M:%S')\n",
    "\n",
    "        status_on_date['status_days'] = ((status_on_date['datenow'] - \n",
    "                                          status_on_date['datefrom'])/np.timedelta64(1, 'D')).astype('int32')\n",
    "\n",
    "        #подсчитаем все статусы, которые были у этой квартиры к указанной дате\n",
    "        statuses_to_date = status[(status.datefrom<=fixed_date)] \\\n",
    "                        .groupby(['id_flatwork_int','stat_new']) \\\n",
    "                        .size() \\\n",
    "                        .reset_index(name = 'cnt_stat_new')\n",
    "        statuses_to_date = statuses_to_date.pivot(index = 'id_flatwork_int', columns='stat_new').fillna(0)\n",
    "        statuses_to_date = my_drop_levels(statuses_to_date, sep = '_') \n",
    "        statuses_to_date = statuses_to_date.reset_index()\n",
    "\n",
    "        #удалим задвоения\n",
    "        tmp = status_on_date.groupby('id_flatwork_int').size().reset_index(name = 'cnt')\n",
    "        flats_to_delete =  tmp.loc[tmp['cnt']>1,'id_flatwork_int']\n",
    "        status_on_date = status_on_date[~status_on_date.id_flatwork_int.isin(flats_to_delete)] \n",
    "\n",
    "        #квартиры, которые могут быть проданы в этом периоде\n",
    "        stats_can_be_sold = dict_stat[dict_stat['can_be_sold']==1].stat\n",
    "        flats_can_be_sold = np.array(status_on_date[status_on_date.stat.isin(stats_can_be_sold)] \\\n",
    "                                     .id_flatwork_int)\n",
    "\n",
    "        #  \n",
    "        stats_sold = dict_stat[dict_stat['sold']==1].stat\n",
    "        flats_sold = status_on_date[status_on_date.stat.isin(stats_sold)].id_flatwork_int\n",
    "        \n",
    "        \n",
    "        flats_returned = np.array(flat[(flat.sale.astype('str')>fixed_date) & \n",
    "                              flat.id_flatwork_int.isin(flats_sold)].id_flatwork_int)\n",
    "        \n",
    "        flats_can_be_sold = np.append(flats_can_be_sold, flats_returned)\n",
    "        \n",
    "        #формируем простейшую поквартирную обучающую выборку\n",
    "        #Если реальная дата продажи > чем начало периода, то даже при статусе реализован, она может быть продана\n",
    "        tmp_flat_train = flat[(~flat.bulk_spalen_id.isna()) & \n",
    "                              (flat.date_salestart <= fixed_date_last) & #ВОТ ТУТ ПОМЕНЯТЬ!!!\n",
    "                              #(flat.flat_salestart <= fixed_date_last) \n",
    "                              (flat.id_flatwork_int.isin(flats_can_be_sold))].copy()\n",
    "\n",
    "        tmp_flat_train['date1']=fixed_date \n",
    "        tmp_flat_train['month_cnt']=i\n",
    "        tmp_flat_train['dt_to_settle'] = ((pd.to_datetime(tmp_flat_train['date1'], \n",
    "                                                          format='%Y-%m-%d')  - \n",
    "                                           tmp_flat_train['date_settle'])/np.timedelta64(1, 'D')).astype('int32')\n",
    "        tmp_flat_train['dt_to_salestart'] = ((pd.to_datetime(tmp_flat_train['date1'], format='%Y-%m-%d') - \n",
    "                                              tmp_flat_train['date_salestart'])/np.timedelta64(1, 'D')).astype('int32')\n",
    "\n",
    "        tmp_flat_train['dt_to_sale'] = ((pd.to_datetime(tmp_flat_train['sale'], \n",
    "                                                        format='%Y-%m-%d') - \n",
    "                                         pd.to_datetime(tmp_flat_train['date1'], \n",
    "                                                        format='%Y-%m-%d'))/np.timedelta64(1, 'D')).astype('int32')  \n",
    "\n",
    "        tmp_flat_train = tmp_flat_train.merge(status_on_date[['id_flatwork_int','stat_new',\n",
    "                                                              'last_stat_new','status_days']], \n",
    "                                              how = 'inner', \n",
    "                                              on = 'id_flatwork_int')\n",
    "        tmp_flat_train = tmp_flat_train.merge(statuses_to_date, how = 'left', on = 'id_flatwork_int')\n",
    "\n",
    "        #цена на дату\n",
    "        price_on_date = price[(price.datefrom<=fixed_date) & (price.dateto>fixed_date)].copy()\n",
    "        price_on_date['datefrom'] = pd.to_datetime(price_on_date['datefrom'], format='%Y-%m-%d')\n",
    "        price_on_date['datenow'] = pd.to_datetime(fixed_date, format='%Y-%m-%d')\n",
    "        #status_on_date['dateto'] = pd.to_datetime(status_on_date['dateto'], format='%Y-%m-%d %H:%M:%S')\n",
    "\n",
    "        price_on_date['price_days'] = ((price_on_date['datenow'] - \n",
    "                                        price_on_date['datefrom'])/np.timedelta64(1, 'D')).astype('int32')\n",
    "\n",
    "\n",
    "        tmp_flat_train = tmp_flat_train.merge(price_on_date[['id_flatwork_int','pricem2',\n",
    "                                                             'last_pricem2','diff_pricem2',\n",
    "                                                             'price_days','was_decrease']], \n",
    "                                              how = 'inner', \n",
    "                                              on = 'id_flatwork_int')\n",
    "\n",
    "\n",
    "        #исторические движения по цене\n",
    "        prices_to_date = price[(price.datefrom<=fixed_date) & (price.pricem2>1)] \\\n",
    "                        .groupby(['id_flatwork_int']) \\\n",
    "                        .agg({'pricem2':('min','max','mean','median','std'),\n",
    "                              'was_decrease':('sum','mean','std')})\n",
    "\n",
    "        prices_to_date = my_drop_levels(prices_to_date, sep = '_') \n",
    "        prices_to_date = prices_to_date.reset_index()\n",
    "        tmp_flat_train = tmp_flat_train.merge(prices_to_date, how = 'left', on = 'id_flatwork_int')\n",
    "\n",
    "\n",
    "        if i==0:\n",
    "            flat_train = tmp_flat_train.fillna(0)\n",
    "        else:\n",
    "            flat_train = flat_train.append(tmp_flat_train.fillna(0))\n",
    "\n",
    "       \n",
    "\n",
    "    flat_train = flat_train.fillna(0) \n",
    "\n",
    "\n",
    "    flat_train['realized_1'] = ((flat_train.dt_to_sale>=0) & \n",
    "                                (flat_train.dt_to_sale<test_days_period[0])).astype('int')\n",
    "    flat_train['realized_2'] = ((flat_train.dt_to_sale>=test_days_period[0]) & \n",
    "                                (flat_train.dt_to_sale<test_days_period[0]+test_days_period[1])).astype('int')           \n",
    "    flat_train['realized_3'] = ((flat_train.dt_to_sale>=test_days_period[0]+test_days_period[1]) & \n",
    "                                (flat_train.dt_to_sale<=test_days_period[0]+test_days_period[1]+test_days_period[2])).astype('int') \n",
    "\n",
    "    flat_train['value_1'] = flat_train['square']*flat_train['realized_1']\n",
    "    flat_train['value_2'] = flat_train['square']*flat_train['realized_2']\n",
    "    flat_train['value_3'] = flat_train['square']*flat_train['realized_3']\n",
    "    \n",
    "    \n",
    "    #может иметь психологический эффект\n",
    "    flat_train['price'] = flat_train['pricem2']*flat_train['square']\n",
    "\n",
    "    tmp = flat_train[(flat_train['pricem2']>1) & (flat_train['stat_new']>0) & (flat_train['stat_new']<5)] \\\n",
    "                    .groupby(['month_cnt','bulk_spalen_id']) \\\n",
    "                    .agg({'price':('min','max','std','count','median'),\n",
    "                          'pricem2':('min','max','std','median')})\n",
    "\n",
    "    tmp = my_drop_levels(tmp, sep = '_', brief = 'bulk_spalen_').reset_index()\n",
    "\n",
    "    flat_train = flat_train.merge(tmp, on = ['month_cnt','bulk_spalen_id'], how = 'left').fillna(0)\n",
    "\n",
    "    del tmp\n",
    "    gc.collect()\n",
    "\n",
    "    flat_train['diff_pricem2_median'] = flat_train['bulk_spalen_pricem2_median']-flat_train['pricem2']\n",
    "    flat_train['diff_price_median'] = flat_train['bulk_spalen_price_median']-flat_train['price']\n",
    "\n",
    "    flat_train['diff_pricem2_min'] = flat_train['pricem2'] - flat_train['bulk_spalen_pricem2_min']\n",
    "    flat_train['diff_price_min'] = flat_train['price'] - flat_train['bulk_spalen_price_min']\n",
    "\n",
    "    flat_train['diff_pricem2_max'] = flat_train['bulk_spalen_pricem2_max']-flat_train['pricem2']\n",
    "    flat_train['diff_price_max'] = flat_train['bulk_spalen_price_max']-flat_train['price']\n",
    "\n",
    "\n",
    "    tmp = flat_train[(flat_train['stat_new']>0) & (flat_train['stat_new']<5)] \\\n",
    "                    .groupby(['month_cnt','bulk_spalen_id']) \\\n",
    "                    .agg({'square':('min','max','std','count','median','mean')})\n",
    "\n",
    "    tmp = my_drop_levels(tmp, sep = '_', brief = 'bulk_spalen_').reset_index()\n",
    "\n",
    "    flat_train = flat_train.merge(tmp, on = ['month_cnt','bulk_spalen_id'], how = 'left').fillna(0)\n",
    "\n",
    "    del tmp\n",
    "    gc.collect()\n",
    "\n",
    "    flat_train['diff_square_median'] = flat_train['bulk_spalen_square_median']-flat_train['square']\n",
    "    flat_train['diff_square_mean'] = flat_train['bulk_spalen_square_mean']-flat_train['square']\n",
    "    flat_train['diff_square_min'] = flat_train['bulk_spalen_square_min']-flat_train['square']\n",
    "    flat_train['diff_square_max'] = flat_train['bulk_spalen_square_max']-flat_train['square']\n",
    "\n",
    "\n",
    "\n",
    "    tmp = flat_train.groupby(['month_cnt','bulk_spalen_id', 'stat_new']) \\\n",
    "                    .size() \\\n",
    "                    .reset_index(name = 'dolya_stat_new')\n",
    "\n",
    "    tmp1 = flat_train.groupby(['month_cnt','bulk_spalen_id']) \\\n",
    "                    .size() \\\n",
    "                    .reset_index(name = 'cnt_stat_new') \n",
    "\n",
    "\n",
    "    tmp1['unique_id'] = tmp1['month_cnt'].astype('str')+'_'+tmp1['bulk_spalen_id'].astype('str')\n",
    "\n",
    "    tmp = tmp.merge(tmp1) \n",
    "    tmp['dolya_stat_new']=tmp['dolya_stat_new']/tmp['cnt_stat_new']\n",
    "\n",
    "\n",
    "    tmp2 = tmp[['unique_id','stat_new','dolya_stat_new']].pivot(index = 'unique_id', columns='stat_new') \\\n",
    "                                                         .fillna(0) \n",
    "\n",
    "    tmp2 = my_drop_levels(tmp2, sep = '_', brief = 'bulk_spalen_').reset_index()\n",
    "    tmp2 = tmp2.merge(tmp1) \n",
    "    #tmp = my_drop_levels(tmp, sep = '_', brief = 'bulk_spalen_').reset_index()\n",
    "\n",
    "    flat_train = flat_train.merge(tmp2.drop('unique_id', axis = 1), on = ['month_cnt','bulk_spalen_id'], how = 'left').fillna(0) \n",
    "\n",
    "    del tmp, tmp1, tmp2\n",
    "    gc.collect()\n",
    "\n",
    "    flat_train['month'] = pd.to_datetime(flat_train.date1, format = '%Y-%m-%d').dt.month\n",
    "    \n",
    "    flat_train = mem_economy(flat_train)\n",
    "    gc.collect()\n",
    "    \n",
    "    \n",
    "    tmp = flat_train.groupby(['month_cnt','bulk_spalen_id', 'stat_new']) \\\n",
    "                    .agg({'square':'sum'}) \\\n",
    "                    .reset_index() \\\n",
    "                    .rename(columns = {'square':'dolya_sqr_stat_new'})\n",
    "\n",
    "    tmp1 = flat_train.groupby(['month_cnt','bulk_spalen_id']) \\\n",
    "                    .agg({'square':'sum'}) \\\n",
    "                    .reset_index() \\\n",
    "                    .rename(columns = {'square':'sqr_stat_new'})\n",
    "\n",
    "\n",
    "    tmp1['unique_id'] = tmp1['month_cnt'].astype('str')+'_'+tmp1['bulk_spalen_id'].astype('str')\n",
    "\n",
    "    tmp = tmp.merge(tmp1) \n",
    "    tmp['dolya_sqr_stat_new']=tmp['dolya_sqr_stat_new']/tmp['sqr_stat_new']\n",
    "\n",
    "\n",
    "    tmp2 = tmp[['unique_id','stat_new','dolya_sqr_stat_new']].pivot(index = 'unique_id', columns='stat_new') \\\n",
    "                                                         .fillna(0) \n",
    "\n",
    "    tmp2 = my_drop_levels(tmp2, sep = '_', brief = 'bulk_spalen_').reset_index()\n",
    "    tmp2 = tmp2.merge(tmp1) \n",
    "    #tmp = my_drop_levels(tmp, sep = '_', brief = 'bulk_spalen_').reset_index()\n",
    "\n",
    "    flat_train = flat_train.merge(tmp2.drop('unique_id', axis = 1), on = ['month_cnt','bulk_spalen_id'], how = 'left').fillna(0) \n",
    "\n",
    "    del tmp, tmp1, tmp2\n",
    "    gc.collect()\n",
    "\n",
    "    return flat_train\n",
    "\n",
    "def my_simple_cv(model, dataset, study_columns, random_state=442, importance_flag = False):\n",
    "    \n",
    "    train_agg = dataset[dataset.is_train==1].copy().reset_index(drop = True)\n",
    "    test_agg = dataset[dataset.is_train==0].copy().reset_index(drop = True)\n",
    "    \n",
    "    ind = 0\n",
    "    _mse = np.array([],dtype = 'float')\n",
    "    #заполним нулями предикт теста\n",
    "    y_test_pred = np.zeros(test_agg.shape[0],dtype = 'float')\n",
    "    \n",
    "    \n",
    "    #основная кросс-валидация\n",
    "    for train_index, valid_index in KFold(n_splits=5, random_state=random_state, shuffle = True).split(train_agg):   \n",
    "\n",
    "        tmp_train  = train_agg.loc[train_index,:]\n",
    "        tmp_valid  = train_agg.loc[valid_index,:]\n",
    "        tmp_test   = test_agg.copy()\n",
    "\n",
    "        #учиться будем только на study_columns не на всех переменных     \n",
    "        X_train = tmp_train.loc[:,study_columns]\n",
    "        X_valid = tmp_valid.loc[:,study_columns]\n",
    "        X_test  = tmp_test.loc[:,study_columns]\n",
    "\n",
    "        y_train = tmp_train['value']\n",
    "        y_valid = tmp_valid['value']\n",
    "        y_test = tmp_test['value'] \n",
    "        \n",
    "                \n",
    "        #обучим модель\n",
    "        model.fit(X_train,y_train)\n",
    " \n",
    "        y_valid_pred = model.predict(X_valid)\n",
    "        y_test_pred = y_test_pred+model.predict(X_test)\n",
    "        \n",
    "        \n",
    "        y_valid_pred[y_valid_pred<0] = 0\n",
    "        \n",
    "        if ind ==0:\n",
    "            stacking_df = pd.DataFrame(dict({'bulk_id_int':tmp_valid.bulk_id_int,'predict':y_valid_pred, 'fact':y_valid}))\n",
    "        else:\n",
    "            tmp_stacking_df = pd.DataFrame(dict({'bulk_id_int':tmp_valid.bulk_id_int,'predict':y_valid_pred, 'fact':y_valid}))\n",
    "            stacking_df = stacking_df.append(tmp_stacking_df).sort_values('bulk_id_int')\n",
    "        \n",
    "        \n",
    "        _mse = np.append(_mse,mean_squared_error(y_valid,y_valid_pred))\n",
    "\n",
    "        ind = ind + 1\n",
    "\n",
    "        #break\n",
    "    \n",
    "    \n",
    "    importance = pd.DataFrame(dict({'feature':'none', 'delta_mse':0}), index = ['none'])\n",
    "    \n",
    "    mse_now = mean_squared_error(y_valid,y_valid_pred)\n",
    "    NUMBER_SHUFFLE = 5\n",
    "    if importance_flag:\n",
    "        for feature in study_columns:\n",
    "\n",
    "            tmp_mse = 0\n",
    "            for i in range(NUMBER_SHUFFLE):\n",
    "                _X_valid = X_valid.copy()\n",
    "                a = np.asarray(X_valid[feature].copy())\n",
    "                np.random.shuffle(a)\n",
    "                _X_valid[feature] = a\n",
    "                y_valid_pred = model.predict(_X_valid)\n",
    "                tmp_mse = tmp_mse+mean_squared_error(y_valid, y_valid_pred)/NUMBER_SHUFFLE\n",
    "            tmp_importance = pd.DataFrame(dict({'feature':feature, 'delta_mse':(tmp_mse-mse_now)}), index = [feature])    \n",
    "            importance = importance.append(tmp_importance) \n",
    "    \n",
    "    \n",
    "    #усредняем по фолдам предсказание теста\n",
    "    y_test_pred = y_test_pred/ind\n",
    "    \n",
    "    y_test_pred[y_test_pred<0] = 0\n",
    "    \n",
    "    submission = pd.DataFrame(dict({'id':test_agg.id,'value':y_test_pred, 'bulk_spalen_id':test_agg.bulk_spalen_id}))\n",
    "    \n",
    "    \n",
    "    \n",
    "    return submission, _mse, stacking_df, importance\n",
    "\n",
    "def my_submit(model, \n",
    "                        dataset, \n",
    "                        right_dataset, \n",
    "                        right_date, \n",
    "                        cv_dates,\n",
    "                        last_date,\n",
    "                        n_month,\n",
    "                        study_columns, \n",
    "                        value_column, \n",
    "                        group_columns, \n",
    "                        random_state=442, \n",
    "                        importance_flag = False):\n",
    "    \n",
    "    #весь обучающий датасет\n",
    "    train_agg = dataset.copy().reset_index(drop = True)\n",
    "    \n",
    "    ind = 0\n",
    "    _mse = np.array([],dtype = 'float')\n",
    "    _grp_mse = np.array([],dtype = 'float')\n",
    "    gc.collect()\n",
    "    print('==========================')\n",
    "    \n",
    "    #основная кросс-валидация\n",
    "    d = cv_dates[len(cv_dates)-1]\n",
    "     \n",
    "    #Расчитаем для submit-а\n",
    "\n",
    "    #обучающая сдвигается на 1 месяц вперед\n",
    "    dt = fixed_dates[d+1]\n",
    "    if d+1-n_month<0:\n",
    "        dt_start = fixed_dates[0]\n",
    "    else:\n",
    "        dt_start = fixed_dates[d+1-n_month]\n",
    "            \n",
    "    print('study dataset fot test: date = ',dt,' dt_start = ',dt_start)\n",
    "        \n",
    "    tmp_train  = train_agg.loc[(train_agg.date1<dt) & (train_agg.date1>=dt_start),:] \n",
    "    #а тестовая - на последнюю известную дату\n",
    "    tmp_test  = train_agg.loc[train_agg.date1==last_date,:]   \n",
    "    tmp_right = right_dataset.loc[right_dataset.date1==right_date,:].copy()\n",
    "        \n",
    "    #учиться будем только на study_columns не на всех переменных     \n",
    "    X_train = tmp_train.loc[:,study_columns]\n",
    "    X_test  = tmp_test.loc[:,study_columns]\n",
    "    \n",
    "    print('Максимальная дата обучающей ',tmp_train.date1.max())\n",
    "    print('Миниимальная дата тестовой ',tmp_test.date1.min())   \n",
    "    \n",
    "    y_train = tmp_train[value_column]\n",
    "        \n",
    "    del tmp_train\n",
    "    gc.collect()\n",
    "        \n",
    "    #обучим модель\n",
    "    model.fit(X_train,y_train)\n",
    "        \n",
    "    y_test_pred = model.predict(X_test)\n",
    "    y_test_pred[y_test_pred<0] = 0\n",
    "        \n",
    "    R_test = X_test.copy()\n",
    "    R_test['predict'] = y_test_pred \n",
    "             \n",
    "    R_test = R_test.groupby(group_columns) \\\n",
    "                             .agg({'predict':'sum'}) \\\n",
    "                             .reset_index()\n",
    "    tmp_right = tmp_right.merge(R_test, on = group_columns, how = 'left')\n",
    "    submission = tmp_right[['id','bulk_spalen_id','predict']].rename(columns = {'predict':'value'}).fillna(0)\n",
    "    \n",
    "\n",
    "    return submission#, _mse, _grp_mse#, importance, model, full_df_for_calc_cv\n",
    "\n",
    "\n",
    "def my_cv(model, \n",
    "                        dataset, \n",
    "                        right_dataset, \n",
    "                        right_date, \n",
    "                        cv_dates,\n",
    "                        last_date,\n",
    "                        n_month,\n",
    "                        study_columns, \n",
    "                        value_column, \n",
    "                        group_columns, \n",
    "                        random_state=442, \n",
    "                        importance_flag = False):\n",
    "    \n",
    "    #весь обучающий датасет\n",
    "    train_agg = dataset.copy().reset_index(drop = True)\n",
    "    \n",
    "    ind = 0\n",
    "    _mse = np.array([],dtype = 'float')\n",
    "    _grp_mse = np.array([],dtype = 'float')\n",
    "    gc.collect()\n",
    "    print('==========================')\n",
    "    \n",
    "    #основная кросс-валидация\n",
    "    for d in cv_dates:\n",
    "        #получаем даты\n",
    "        dt = fixed_dates[d]\n",
    "        if d-n_month<0:\n",
    "            dt_start = fixed_dates[0]\n",
    "        else:\n",
    "            dt_start = fixed_dates[d-n_month]\n",
    "            \n",
    "        print('ind = ',ind, ' date = ',dt,' dt_start = ',dt_start)\n",
    "        \n",
    "        tmp_train  = train_agg.loc[(train_agg.date1<dt) & (train_agg.date1>=dt_start),:]   \n",
    "        tmp_valid  = train_agg.loc[train_agg.date1==dt,:]\n",
    "        tmp_right = right_dataset.loc[right_dataset.date1==dt,:].copy()\n",
    "        \n",
    "        #учиться будем только на study_columns не на всех переменных     \n",
    "        X_train = tmp_train.loc[:,study_columns]\n",
    "        X_valid = tmp_valid.loc[:,study_columns]\n",
    "        \n",
    "        y_train = tmp_train[value_column]\n",
    "        y_valid = tmp_valid[value_column]\n",
    "        \n",
    "        del tmp_train#, tmp_valid\n",
    "        gc.collect()\n",
    "        \n",
    "        #обучим модель\n",
    "        model.fit(X_train,y_train)\n",
    "        \n",
    "        y_valid_pred = model.predict(X_valid)\n",
    "        y_valid_pred[y_valid_pred<0] = 0\n",
    "        \n",
    "        R_valid = tmp_valid[['bulk_spalen_id','id_flatwork_int']].copy() #X_valid.copy()\n",
    "        R_valid['predict'] = y_valid_pred\n",
    "        \n",
    "        print(f'X_valid.shape = {X_valid.shape:}')\n",
    "        _mse = np.append(_mse,mean_squared_error(y_valid,y_valid_pred))\n",
    "\n",
    "            \n",
    "        R_valid['value_flat'] = y_valid\n",
    "\n",
    "        R_valid = R_valid.groupby(group_columns) \\\n",
    "                             .agg({'predict':'sum','value_flat':'sum'}) \\\n",
    "                             .reset_index()\n",
    "        tmp_right = tmp_right.merge(R_valid, on = group_columns, how = 'left').fillna(0)\n",
    "            \n",
    "        if 1==0:\n",
    "            if ind == 0:\n",
    "                full_df_for_calc_cv = tmp_right[['value','predict']].copy()\n",
    "                full_df_for_calc_cv['ind'] = ind\n",
    "            else:\n",
    "                tmp_df_for_calc_cv = tmp_right[['value','predict']].copy()\n",
    "                tmp_df_for_calc_cv['ind'] = ind\n",
    "                full_df_for_calc_cv = full_df_for_calc_cv.append(tmp_df_for_calc_cv)\n",
    "\n",
    "            _grp_mse = np.append(_grp_mse,mean_squared_error(tmp_right['value'],tmp_right['predict']))\n",
    "\n",
    "        ind = ind + 1\n",
    "\n",
    "        #break\n",
    "        \n",
    "        \n",
    "    #посчитаем важность\n",
    "    importance = pd.DataFrame(dict({'feature':'none', 'delta_mse':0}), index = ['none'])\n",
    "    \n",
    "    mse_now = mean_squared_error(y_valid,y_valid_pred)\n",
    "    NUMBER_SHUFFLE = 5\n",
    "    if importance_flag:\n",
    "        for feature in study_columns:\n",
    "\n",
    "            tmp_mse = 0\n",
    "            for i in range(NUMBER_SHUFFLE):\n",
    "                _X_valid = X_valid.copy()\n",
    "                a = np.asarray(X_valid[feature].copy())\n",
    "                np.random.shuffle(a)\n",
    "                _X_valid[feature] = a\n",
    "                y_valid_pred = model.predict(_X_valid)\n",
    "                tmp_mse = tmp_mse+mean_squared_error(y_valid, y_valid_pred)/NUMBER_SHUFFLE\n",
    "            tmp_importance = pd.DataFrame(dict({'feature':feature, 'delta_mse':(tmp_mse-mse_now)}), index = [feature])    \n",
    "            importance = importance.append(tmp_importance)     \n",
    "    \n",
    "    #Расчитаем для submit-а\n",
    "    \n",
    "    #обучающая сдвигается на 1 месяц вперед\n",
    "    dt = fixed_dates[d+1]\n",
    "    if d+1-n_month<0:\n",
    "        dt_start = fixed_dates[0]\n",
    "    else:\n",
    "        dt_start = fixed_dates[d+1-n_month]\n",
    "            \n",
    "    print('study dataset fot test: date = ',dt,' dt_start = ',dt_start)\n",
    "        \n",
    "    tmp_train  = train_agg.loc[(train_agg.date1<dt) & (train_agg.date1>=dt_start),:] \n",
    "    #а тестовая - на последнюю известную дату\n",
    "    tmp_test  = train_agg.loc[train_agg.date1==last_date,:]   \n",
    "    tmp_right = right_dataset.loc[right_dataset.date1==right_date,:].copy()\n",
    "        \n",
    "    #учиться будем только на study_columns не на всех переменных     \n",
    "    X_train = tmp_train.loc[:,study_columns]\n",
    "    X_test  = tmp_test.loc[:,study_columns]\n",
    "        \n",
    "    y_train = tmp_train[value_column]\n",
    "        \n",
    "    del tmp_train\n",
    "    gc.collect()\n",
    "        \n",
    "    #обучим модель\n",
    "    model.fit(X_train,y_train)\n",
    "        \n",
    "    y_test_pred = model.predict(X_test)\n",
    "    y_test_pred[y_test_pred<0] = 0\n",
    "        \n",
    "    R_test = tmp_test[['bulk_spalen_id','id_flatwork_int']].copy() #X_test.copy()\n",
    "    R_test['predict'] = y_test_pred \n",
    "             \n",
    "    R_test = R_test.groupby(group_columns) \\\n",
    "                             .agg({'predict':'sum'}) \\\n",
    "                             .reset_index()\n",
    "    tmp_right = tmp_right.merge(R_test, on = group_columns, how = 'left')\n",
    "    submission = tmp_right[['id','predict']].rename(columns = {'predict':'value'}).fillna(0)\n",
    "    \n",
    "\n",
    "    return submission, _mse, _grp_mse, importance, model#, full_df_for_calc_cv\n",
    "\n",
    "\n",
    "##############################\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:37: FutureWarning: Sorting because non-concatenation axis is not aligned. A future version\n",
      "of pandas will change to not sort by default.\n",
      "\n",
      "To accept the future behavior, pass 'sort=False'.\n",
      "\n",
      "To retain the current behavior and silence the warning, pass 'sort=True'.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2.24 s, sys: 8 ms, total: 2.24 s\n",
      "Wall time: 2.24 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "full = prepare_full()\n",
    "flat, dict_bulk_spalen, dict_flat = prepare_flat()\n",
    "\n",
    "full = mem_economy(full)\n",
    "gc.collect()\n",
    "flat = mem_economy(flat)\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. возьмем за валидацию 3 месяца (ноябрь,декабрь,январь)\n",
    "# попробуем сымитировать то, что происходит на привате\n",
    "valid_date = '2017-12-01'#'2017-03-01'\n",
    "valid_date_end = '2018-02-01'#'2017-05-01'\n",
    "#valid_month_cnt = [35, 36, 37]\n",
    "test_month_cnt = [38, 39, 40]\n",
    "\n",
    "#Добавим данные о максимальной площади, доступной для продажи\n",
    "max_sale_square = flat[flat['sale'].astype('str')>valid_date].groupby('bulk_spalen_id') \\\n",
    "                                                             .square.sum() \\\n",
    "                                                             .reset_index(name = 'max_square')\n",
    "\n",
    "full = full.merge(max_sale_square, on = 'bulk_spalen_id', how = 'left').fillna(0)\n",
    "\n",
    "\n",
    "\n",
    "test = full[full['month_cnt'].isin(test_month_cnt)].copy()\n",
    "full = full[(full['date1']<=valid_date_end)]\n",
    "\n",
    "full.loc[(full['date1']>=valid_date) & (full['date1']<=valid_date_end), 'is_train'] = 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1709, 65)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "full[full['is_train']==0].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Удалим те, по которым не было продаж все 3 месяца валидационной выборки"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(9244, 65)\n",
      "(8568, 65)\n"
     ]
    }
   ],
   "source": [
    "tmp =  full[full['is_train']==0].groupby('bulk_spalen_id').value.min().reset_index(name = 'value')\n",
    "bulk_spalen_id_to_delete = np.array(tmp[tmp['value']==0].bulk_spalen_id)\n",
    "\n",
    "print(full.shape)\n",
    "full = full[(~full.bulk_spalen_id.isin(bulk_spalen_id_to_delete)) | (full.is_train==1)]\n",
    "print(full.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 10.6 s, sys: 416 ms, total: 11 s\n",
      "Wall time: 11 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "#valid_date = '2017-11-01'\n",
    "\n",
    "status, dict_stat = prepare_status()\n",
    "price             = prepare_price()\n",
    "\n",
    "#status = mem_economy(status)\n",
    "#price  = mem_economy(price)\n",
    "\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Формируем поквартирную обучающую выборку"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test_days_period = [31 31 28]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/.local/lib/python3.6/site-packages/pandas/core/frame.py:6211: FutureWarning: Sorting because non-concatenation axis is not aligned. A future version\n",
      "of pandas will change to not sort by default.\n",
      "\n",
      "To accept the future behavior, pass 'sort=False'.\n",
      "\n",
      "To retain the current behavior and silence the warning, pass 'sort=True'.\n",
      "\n",
      "  sort=sort)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 54.1 s, sys: 4.97 s, total: 59.1 s\n",
      "Wall time: 59 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "tmp_calendar = full.loc[full['is_train']==0,('date1','date2')].sort_values('date1').drop_duplicates()\n",
    "test_days_period =  np.array(((tmp_calendar.date2 - tmp_calendar.date1)/np.timedelta64(1,'D')+1).astype('int32')) \n",
    "\n",
    "print(f'test_days_period = {test_days_period:}')\n",
    "flat_train = prepare_flat_train(test_days_period)\n",
    "\n",
    "flat_train = mem_economy(flat_train)\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# train и test model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'predict'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36mget_loc\u001b[0;34m(self, key, method, tolerance)\u001b[0m\n\u001b[1;32m   3063\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3064\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3065\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'predict'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-23-33f6a01cde70>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0mrmse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m \u001b[0msubmission_lgb\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'error'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0msubmission_lgb\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'value'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0msubmission_lgb\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'predict'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m \u001b[0msubmission_lgb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msubmission_lgb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msort_values\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'id'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0mfilename\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34mf'results/lgb_simulate_{(np.mean(rmse)):.4f} +- {(np.std(rmse)):.4f}.csv'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   2686\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_getitem_multilevel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2687\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2688\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_getitem_column\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2689\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2690\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_getitem_column\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m_getitem_column\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   2693\u001b[0m         \u001b[0;31m# get column\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2694\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_unique\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2695\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_item_cache\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2696\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2697\u001b[0m         \u001b[0;31m# duplicate columns & possible reduce dimensionality\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/pandas/core/generic.py\u001b[0m in \u001b[0;36m_get_item_cache\u001b[0;34m(self, item)\u001b[0m\n\u001b[1;32m   2484\u001b[0m         \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcache\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2485\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mres\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2486\u001b[0;31m             \u001b[0mvalues\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2487\u001b[0m             \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_box_item_values\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalues\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2488\u001b[0m             \u001b[0mcache\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mres\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/pandas/core/internals.py\u001b[0m in \u001b[0;36mget\u001b[0;34m(self, item, fastpath)\u001b[0m\n\u001b[1;32m   4113\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4114\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misna\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4115\u001b[0;31m                 \u001b[0mloc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   4116\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4117\u001b[0m                 \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0misna\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36mget_loc\u001b[0;34m(self, key, method, tolerance)\u001b[0m\n\u001b[1;32m   3064\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3065\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3066\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maybe_cast_indexer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3067\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3068\u001b[0m         \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_indexer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmethod\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtolerance\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtolerance\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'predict'"
     ]
    }
   ],
   "source": [
    "full['calc_last_value'] = full['calc_last_value'].fillna(0)\n",
    "full = full.reset_index(drop  = True)\n",
    "\n",
    "column_study = np.array(['До метро пешком(км)', 'price', 'mean_sq', 'price_by_square',\n",
    "       'mean_fl', 'Cтавка по ипотеке', 'Станций метро от кольца',\n",
    "       'Площадь двора', 'Date_int', 'До промки(км)', 'month',\n",
    "       'До большой дороги на машине(км)', 'spalen',\n",
    "       'Площадь зеленой зоны в радиусе 500 м', 'bulk_id_int',\n",
    "       'До удобной авторазвязки на машине(км)', 'До парка пешком(км)',\n",
    "       'Курс', 'До Кремля', 'Вклады свыше 3 лет','calc_last_value'])\n",
    "\n",
    "lgb_model = lgb.LGBMRegressor(n_estimators = 150, random_state = 42)\n",
    "\n",
    "submission_lgb, mse, stacking_df, imp_df = my_simple_cv(lgb_model, \n",
    "                                                        full, \n",
    "                                                        column_study, \n",
    "                                                        random_state=442, \n",
    "                                                        importance_flag = True)\n",
    "\n",
    "#rmse на локальной валидации этой модели\n",
    "rmse = np.sqrt(mse)\n",
    "\n",
    "submission_lgb['error'] = (submission_lgb['value']-submission_lgb['predict'])**2\n",
    "submission_lgb = submission_lgb.sort_values('id')\n",
    "filename = f'results/lgb_simulate_{(np.mean(rmse)):.4f} +- {(np.std(rmse)):.4f}.csv'\n",
    "#submission_lgb.to_csv(filename, index = False)\n",
    "\n",
    "#rmse на реалистичной валидации\n",
    "rmse_real = np.sqrt(mean_squared_error(submission_lgb.value,submission_lgb.predict))\n",
    "\n",
    "print(f'rmse_algo = {np.mean(rmse)+np.std(rmse):.4f} rmse_real = {rmse_real:.4f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imp_df.sort_values('delta_mse', ascending = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 393,
   "metadata": {},
   "outputs": [],
   "source": [
    "#rmse_algo = 235.9872 rmse_real = 284.8411"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rmse_on_new = 274.8612 rmse_on_old = 252.6956 rmse_on_all = 256.5499\n"
     ]
    }
   ],
   "source": [
    "rmse_on_new = np.sqrt(np.mean(submission_lgb[submission_lgb.bulk_spalen_id.isin(only_valid_tmp_ids)].error))\n",
    "rmse_on_old = np.sqrt(np.mean(submission_lgb[~submission_lgb.bulk_spalen_id.isin(only_valid_tmp_ids)].error))\n",
    "rmse_on_all = np.sqrt(np.mean(submission_lgb.error))\n",
    "\n",
    "print(f'rmse_on_new = {rmse_on_new:.4f} rmse_on_old = {rmse_on_old:.4f} rmse_on_all = {rmse_on_all:.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Поквартирная модель"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def my_submit(model, \n",
    "                        dataset, \n",
    "                        right_dataset, \n",
    "                        right_date, \n",
    "                        cv_dates,\n",
    "                        last_date,\n",
    "                        n_month,\n",
    "                        study_columns, \n",
    "                        value_column, \n",
    "                        group_columns, \n",
    "                        random_state=442, \n",
    "                        importance_flag = False):\n",
    "    \n",
    "    #весь обучающий датасет\n",
    "    train_agg = dataset.copy().reset_index(drop = True)\n",
    "    \n",
    "    ind = 0\n",
    "    _mse = np.array([],dtype = 'float')\n",
    "    _grp_mse = np.array([],dtype = 'float')\n",
    "    gc.collect()\n",
    "    print('==========================')\n",
    "    \n",
    "    #основная кросс-валидация\n",
    "    d = cv_dates[len(cv_dates)-1]\n",
    "     \n",
    "    #Расчитаем для submit-а\n",
    "\n",
    "    #обучающая сдвигается на 1 месяц вперед\n",
    "    dt = fixed_dates[d+1]\n",
    "    if d+1-n_month<0:\n",
    "        dt_start = fixed_dates[0]\n",
    "    else:\n",
    "        dt_start = fixed_dates[d+1-n_month]\n",
    "            \n",
    "    print('study dataset fot test: date = ',dt,' dt_start = ',dt_start)\n",
    "        \n",
    "    tmp_train  = train_agg.loc[(train_agg.date1<dt) & (train_agg.date1>=dt_start),:] \n",
    "    #а тестовая - на последнюю известную дату\n",
    "    tmp_test  = train_agg.loc[train_agg.date1==last_date,:]   \n",
    "    tmp_right = right_dataset.loc[right_dataset.date1==right_date,:].copy()\n",
    "        \n",
    "    #учиться будем только на study_columns не на всех переменных     \n",
    "    X_train = tmp_train.loc[:,study_columns]\n",
    "    X_test  = tmp_test.loc[:,study_columns]\n",
    "    \n",
    "    print('Максимальная дата обучающей ',tmp_train.date1.max())\n",
    "    print('Миниимальная дата тестовой ',tmp_test.date1.min())   \n",
    "    \n",
    "    y_train = tmp_train[value_column]\n",
    "        \n",
    "    del tmp_train\n",
    "    gc.collect()\n",
    "        \n",
    "    #обучим модель\n",
    "    model.fit(X_train,y_train)\n",
    "        \n",
    "    y_test_pred = model.predict(X_test)\n",
    "    y_test_pred[y_test_pred<0] = 0\n",
    "        \n",
    "    R_test = X_test.copy()\n",
    "    R_test['predict'] = y_test_pred \n",
    "             \n",
    "    R_test = R_test.groupby(group_columns) \\\n",
    "                             .agg({'predict':'sum'}) \\\n",
    "                             .reset_index()\n",
    "    tmp_right = tmp_right.merge(R_test, on = group_columns, how = 'left')\n",
    "    submission = tmp_right[['id','bulk_spalen_id','predict']].rename(columns = {'predict':'value'}).fillna(0)\n",
    "    \n",
    "\n",
    "    return submission#, _mse, _grp_mse#, importance, model, full_df_for_calc_cv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def my_cv(model, \n",
    "                        dataset, \n",
    "                        right_dataset, \n",
    "                        right_date, \n",
    "                        cv_dates,\n",
    "                        last_date,\n",
    "                        n_month,\n",
    "                        study_columns, \n",
    "                        value_column, \n",
    "                        group_columns, \n",
    "                        random_state=442, \n",
    "                        importance_flag = False):\n",
    "    \n",
    "    #весь обучающий датасет\n",
    "    train_agg = dataset.copy().reset_index(drop = True)\n",
    "    \n",
    "    ind = 0\n",
    "    _mse = np.array([],dtype = 'float')\n",
    "    _grp_mse = np.array([],dtype = 'float')\n",
    "    gc.collect()\n",
    "    print('==========================')\n",
    "    \n",
    "    #основная кросс-валидация\n",
    "    for d in cv_dates:\n",
    "        #получаем даты\n",
    "        dt = fixed_dates[d]\n",
    "        if d-n_month<0:\n",
    "            dt_start = fixed_dates[0]\n",
    "        else:\n",
    "            dt_start = fixed_dates[d-n_month]\n",
    "            \n",
    "        print('ind = ',ind, ' date = ',dt,' dt_start = ',dt_start)\n",
    "        \n",
    "        tmp_train  = train_agg.loc[(train_agg.date1<dt) & (train_agg.date1>=dt_start),:]   \n",
    "        tmp_valid  = train_agg.loc[train_agg.date1==dt,:]\n",
    "        tmp_right = right_dataset.loc[right_dataset.date1==dt,:].copy()\n",
    "        \n",
    "        #учиться будем только на study_columns не на всех переменных     \n",
    "        X_train = tmp_train.loc[:,study_columns]\n",
    "        X_valid = tmp_valid.loc[:,study_columns]\n",
    "        \n",
    "        y_train = tmp_train[value_column]\n",
    "        y_valid = tmp_valid[value_column]\n",
    "        \n",
    "        del tmp_train#, tmp_valid\n",
    "        gc.collect()\n",
    "        \n",
    "        #обучим модель\n",
    "        model.fit(X_train,y_train)\n",
    "        \n",
    "        y_valid_pred = model.predict(X_valid)\n",
    "        y_valid_pred[y_valid_pred<0] = 0\n",
    "        \n",
    "        R_valid = tmp_valid[['bulk_spalen_id','id_flatwork_int']].copy() #X_valid.copy()\n",
    "        R_valid['predict'] = y_valid_pred\n",
    "        \n",
    "        print(f'X_valid.shape = {X_valid.shape:}')\n",
    "        _mse = np.append(_mse,mean_squared_error(y_valid,y_valid_pred))\n",
    "\n",
    "            \n",
    "        R_valid['value_flat'] = y_valid\n",
    "\n",
    "        R_valid = R_valid.groupby(group_columns) \\\n",
    "                             .agg({'predict':'sum','value_flat':'sum'}) \\\n",
    "                             .reset_index()\n",
    "        tmp_right = tmp_right.merge(R_valid, on = group_columns, how = 'left').fillna(0)\n",
    "            \n",
    "        if 1==0:\n",
    "            if ind == 0:\n",
    "                full_df_for_calc_cv = tmp_right[['value','predict']].copy()\n",
    "                full_df_for_calc_cv['ind'] = ind\n",
    "            else:\n",
    "                tmp_df_for_calc_cv = tmp_right[['value','predict']].copy()\n",
    "                tmp_df_for_calc_cv['ind'] = ind\n",
    "                full_df_for_calc_cv = full_df_for_calc_cv.append(tmp_df_for_calc_cv)\n",
    "\n",
    "            _grp_mse = np.append(_grp_mse,mean_squared_error(tmp_right['value'],tmp_right['predict']))\n",
    "\n",
    "        ind = ind + 1\n",
    "\n",
    "        #break\n",
    "        \n",
    "        \n",
    "    #посчитаем важность\n",
    "    importance = pd.DataFrame(dict({'feature':'none', 'delta_mse':0}), index = ['none'])\n",
    "    \n",
    "    mse_now = mean_squared_error(y_valid,y_valid_pred)\n",
    "    NUMBER_SHUFFLE = 5\n",
    "    if importance_flag:\n",
    "        for feature in study_columns:\n",
    "\n",
    "            tmp_mse = 0\n",
    "            for i in range(NUMBER_SHUFFLE):\n",
    "                _X_valid = X_valid.copy()\n",
    "                a = np.asarray(X_valid[feature].copy())\n",
    "                np.random.shuffle(a)\n",
    "                _X_valid[feature] = a\n",
    "                y_valid_pred = model.predict(_X_valid)\n",
    "                tmp_mse = tmp_mse+mean_squared_error(y_valid, y_valid_pred)/NUMBER_SHUFFLE\n",
    "            tmp_importance = pd.DataFrame(dict({'feature':feature, 'delta_mse':(tmp_mse-mse_now)}), index = [feature])    \n",
    "            importance = importance.append(tmp_importance)     \n",
    "    \n",
    "    #Расчитаем для submit-а\n",
    "    \n",
    "    #обучающая сдвигается на 1 месяц вперед\n",
    "    dt = fixed_dates[d+1]\n",
    "    if d+1-n_month<0:\n",
    "        dt_start = fixed_dates[0]\n",
    "    else:\n",
    "        dt_start = fixed_dates[d+1-n_month]\n",
    "            \n",
    "    print('study dataset fot test: date = ',dt,' dt_start = ',dt_start)\n",
    "        \n",
    "    tmp_train  = train_agg.loc[(train_agg.date1<dt) & (train_agg.date1>=dt_start),:] \n",
    "    #а тестовая - на последнюю известную дату\n",
    "    tmp_test  = train_agg.loc[train_agg.date1==last_date,:]   \n",
    "    tmp_right = right_dataset.loc[right_dataset.date1==right_date,:].copy()\n",
    "        \n",
    "    #учиться будем только на study_columns не на всех переменных     \n",
    "    X_train = tmp_train.loc[:,study_columns]\n",
    "    X_test  = tmp_test.loc[:,study_columns]\n",
    "        \n",
    "    y_train = tmp_train[value_column]\n",
    "        \n",
    "    del tmp_train\n",
    "    gc.collect()\n",
    "        \n",
    "    #обучим модель\n",
    "    model.fit(X_train,y_train)\n",
    "        \n",
    "    y_test_pred = model.predict(X_test)\n",
    "    y_test_pred[y_test_pred<0] = 0\n",
    "        \n",
    "    R_test = tmp_test[['bulk_spalen_id','id_flatwork_int']].copy() #X_test.copy()\n",
    "    R_test['predict'] = y_test_pred \n",
    "             \n",
    "    R_test = R_test.groupby(group_columns) \\\n",
    "                             .agg({'predict':'sum'}) \\\n",
    "                             .reset_index()\n",
    "    tmp_right = tmp_right.merge(R_test, on = group_columns, how = 'left')\n",
    "    submission = tmp_right[['id','predict']].rename(columns = {'predict':'value'}).fillna(0)\n",
    "    \n",
    "\n",
    "    return submission, _mse, _grp_mse, importance, model#, full_df_for_calc_cv\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==========================\n",
      "study dataset fot test: date =  2017-09-01  dt_start =  2016-12-01\n",
      "Максимальная дата обучающей  2017-08-01\n",
      "Миниимальная дата тестовой  2017-09-01\n",
      "==========================\n",
      "study dataset fot test: date =  2017-08-01  dt_start =  2016-11-01\n",
      "Максимальная дата обучающей  2017-07-01\n",
      "Миниимальная дата тестовой  2017-09-01\n",
      "==========================\n",
      "study dataset fot test: date =  2017-07-01  dt_start =  2016-10-01\n",
      "Максимальная дата обучающей  2017-06-01\n",
      "Миниимальная дата тестовой  2017-09-01\n",
      "CPU times: user 2min 3s, sys: 16.5 s, total: 2min 19s\n",
      "Wall time: 40.1 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "fixed_dates = np.sort(full.date1.dt.strftime('%Y-%m-%d').unique())\n",
    "\n",
    "\n",
    "gc.collect()\n",
    "#определим цену продажи\n",
    "column_filter = ['id_sec','id_gk','id_flatwork','date_settle', \n",
    "                 'date_salestart','sale','bulk_id',\n",
    "                 'date1','realized_1', 'realized_2', 'realized_3',\n",
    "                 'value_1','value_2', 'value_3','dt_to_sale']\n",
    "\n",
    "                \n",
    "column_study = np.setdiff1d(np.asarray(flat_train.columns), column_filter)\n",
    "\n",
    "\n",
    "#n_month = 15\n",
    "#last_date = '2018-02-01'\n",
    "last_date = valid_date\n",
    "\n",
    "for i in range(1):\n",
    "    n_month = 15\n",
    "    lgb_model = lgb.LGBMRegressor(n_estimators = 200, random_state = 42+i, predict_leaf_index = True)\n",
    "    \n",
    "    submission_1 = my_submit(\n",
    "                            model = lgb_model, \n",
    "                            dataset = flat_train,\n",
    "                            right_dataset = full[['id','is_train','bulk_spalen_id','value','date1']],\n",
    "                            right_date = fixed_dates[-3], \n",
    "                            cv_dates = [len(fixed_dates)-4], \n",
    "                            last_date = last_date,\n",
    "                            n_month = n_month,\n",
    "                            study_columns = column_study, \n",
    "                            value_column = 'value_1', \n",
    "                            group_columns = 'bulk_spalen_id',\n",
    "                            random_state=442, \n",
    "                            importance_flag = True)\n",
    "\n",
    "    submission_2 = my_submit(\n",
    "                            model = lgb_model, \n",
    "                            dataset = flat_train,\n",
    "                            right_dataset = full[['id','is_train','bulk_spalen_id','value','date1']],\n",
    "                            right_date = fixed_dates[-2], \n",
    "                            cv_dates = [len(fixed_dates)-5],\n",
    "                            last_date = last_date,\n",
    "                            n_month = n_month,\n",
    "                            study_columns = column_study, \n",
    "                            value_column = 'value_2', \n",
    "                            group_columns = 'bulk_spalen_id',\n",
    "                            random_state=442, \n",
    "                            importance_flag = True)\n",
    "\n",
    "    submission_3 = my_submit(\n",
    "                            model = lgb_model, \n",
    "                            dataset = flat_train,\n",
    "                            right_dataset = full[['id','is_train','bulk_spalen_id','value','date1']],\n",
    "                            right_date = fixed_dates[-1],\n",
    "                            cv_dates = [len(fixed_dates)-6],\n",
    "                            last_date = last_date,\n",
    "                            n_month = n_month,\n",
    "                            study_columns = column_study, \n",
    "                            value_column = 'value_3', \n",
    "                            group_columns = 'bulk_spalen_id',\n",
    "                            random_state=442, \n",
    "                            importance_flag = True)\n",
    "\n",
    "\n",
    "    submission_flat = pd.concat([submission_1,submission_2,submission_3]).fillna(0).sort_values('id')\n",
    "    \n",
    "    if i==0:\n",
    "        v = submission_flat['value']\n",
    "    else: \n",
    "        v = v + submission_flat['value']\n",
    "        \n",
    "submission_flat['value'] = v/(i+1)  \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission_flat = submission_flat.sort_values('id').reset_index(drop = True)\n",
    "submission_flat['predict'] = submission_flat['value']\n",
    "submission_flat['value'] = np.array(submission_lgb.sort_values('id')['value'])#.reset_index(drop = True)\n",
    "\n",
    "submission_flat['error'] = (submission_flat['value']-submission_flat['predict'])**2\n",
    "\n",
    "filename = f'results/x_{(i+1):}_nmonth_15.csv'\n",
    "#submission_flat.to_csv(filename, index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rmse_on_new = 362.5650 rmse_on_old = 281.4523 rmse_on_all = 296.6203\n"
     ]
    }
   ],
   "source": [
    "rmse_on_new = np.sqrt(np.mean(submission_flat[submission_flat.bulk_spalen_id.isin(only_valid_tmp_ids)].error))\n",
    "rmse_on_old = np.sqrt(np.mean(submission_flat[~submission_flat.bulk_spalen_id.isin(only_valid_tmp_ids)].error))\n",
    "rmse_on_all = np.sqrt(np.mean(submission_flat.error))\n",
    "\n",
    "print(f'rmse_on_new = {rmse_on_new:.4f} rmse_on_old = {rmse_on_old:.4f} rmse_on_all = {rmse_on_all:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "#submission_flat[submission_flat.bulk_spalen_id.isin(only_valid_tmp_ids)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1126, 5)\n",
      "(1126, 5)\n",
      "c = 0.00 min_rmse = 260.2869 \n",
      "c = 0.05 min_rmse = 258.7759 \n",
      "c = 0.10 min_rmse = 257.5589 \n",
      "c = 0.15 min_rmse = 256.6403 \n",
      "c = 0.20 min_rmse = 256.0232 \n",
      "c = 0.25 min_rmse = 255.7097 \n",
      "c = 0.30 min_rmse = 255.7011 \n"
     ]
    }
   ],
   "source": [
    "best_lgb = submission_lgb\n",
    "best_flat = submission_flat\n",
    "\n",
    "print(best_lgb.shape)\n",
    "print(best_flat.shape)\n",
    "\n",
    "best = best_flat.merge(best_lgb, on = ['id','bulk_spalen_id'], how = 'left')\n",
    "c = 0.75\n",
    "\n",
    "min_rmse = 999\n",
    "\n",
    "for c in np.arange(0,1,0.05):\n",
    "\n",
    "    best['predict'] = best['predict_x']\n",
    "    best.loc[best['predict_x']==0, 'predict'] = best.loc[best['predict_x']==0, 'predict_y']\n",
    "    best['predict'] = c*best['predict']+(1-c)*best['predict_y']\n",
    "    \n",
    "    rmse_blend = np.sqrt(mean_squared_error(best['value_x'], best['predict']))\n",
    "    if rmse_blend<min_rmse:\n",
    "        min_rmse = rmse_blend\n",
    "        mic_c = c\n",
    "        print(f'c = {c:.2f} min_rmse = {min_rmse:.4f} ' )\n",
    "\n",
    "best['predict'] = best['predict_x']\n",
    "best.loc[best['predict_x']==0, 'predict'] = best.loc[best['predict_x']==0, 'predict_y']\n",
    "best['predict'] = mic_c*best['predict']+(1-mic_c)*best['predict_y']\n",
    "     \n",
    "           \n",
    "#filename = f'results/blend_auto_v1.csv'\n",
    "#best[['id','value']].to_csv(filename, index = False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1126, 5)\n",
      "(1126, 5)\n"
     ]
    }
   ],
   "source": [
    "#best_lgb = submission_lgb\n",
    "#best_flat = submission_flat\n",
    "\n",
    "#print(best_lgb.shape)\n",
    "#print(best_flat.shape)\n",
    "\n",
    "#best = best_flat.merge(best_lgb, on = ['id','bulk_spalen_id'], how = 'left')\n",
    "#mic_c = 0.6\n",
    "\n",
    "#best['predict'] = best['predict_x']\n",
    "#best.loc[best['predict_x']==0, 'predict'] = best.loc[best['predict_x']==0, 'predict_y']\n",
    "#best['predict'] = mic_c*best['predict']+(1-mic_c)*best['predict_y']\n",
    "     \n",
    "           \n",
    "#filename = f'results/blend_auto_v1.csv'\n",
    "#best[['id','value']].to_csv(filename, index = False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Добавим информацию по максимальной площади, доступной для продажи"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Добавим знания о квартирах\n",
    "max_sale_square = flat[flat['sale'].astype('str')>valid_date].groupby('bulk_spalen_id').agg({'square':'sum'}).reset_index()\n",
    "res_sale_square = best.groupby('bulk_spalen_id').agg({'predict':'sum'}).reset_index()\n",
    "\n",
    "res_sale_square = res_sale_square.merge(max_sale_square, on = 'bulk_spalen_id', how = 'left').fillna(0)\n",
    "res_sale_square['coeff'] = res_sale_square['square']/res_sale_square['predict']\n",
    "res_sale_square.loc[res_sale_square['coeff']>1, 'coeff'] = 1\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rmse_blend =  261.7682306960923\n"
     ]
    }
   ],
   "source": [
    "best = best.merge(res_sale_square[['bulk_spalen_id','coeff']], how = 'left', on = 'bulk_spalen_id')\n",
    "best['predict'] = best['predict']*best['coeff']\n",
    "\n",
    "rmse_blend = np.sqrt(mean_squared_error(best['value_x'], best['predict']))\n",
    "print(f'rmse_blend = ',rmse_blend)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
